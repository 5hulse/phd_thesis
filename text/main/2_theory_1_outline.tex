\section{Outline of the Problem}
\label{sec:theory-outline}
For the purposes of this work, it is always assumed that an \ac{FID} to be
estimated
$\bY \in \mathbb{C}^{\None \times \cdots \times \ND}$
is hypercomplex in form, meaning that it obeys
\eqref{eq:general-fid} with $\zeta^{(d)} = \exp(\iu \cdot)\ \forall d \in
\lbrace 1, \cdots D \rbrace$:
\begin{subequations}
    \begin{gather}
        \ynonenD = \xnonenD(\bth) + \wnonenD,\\
        \xnonenD(\bth) =
        \sumM \amexpphim
        \prodD \exp\left(\left(
            2 \pi \iu \left(f^{(d)}_m - \foffd\right)
            -\eta_m\right)
            \nd \Dtd\right),\\
            \label{eq:x}
        \wnonenD \sim \mathcal{N_C}\left(0, 2\sigma^2\right),
    \end{gather}
    \label{eq:hypercomplex-fid}
\end{subequations}%
where $\Dtd = \nicefrac{1}{\fswd}$. Under this model, it is assumed that
\iac{FID} consists of a summation of $M$ damped complex sinusoids in the
presence in \ac{AWGN}.
It is the goal of parametric estimation to establish the
identity of all the quantities which describe the model component $\bX$, which
are distilled into the vector $\bth \in \mathbb{R}^{2(D + 2)M}$, given by
\eqref{eq:theta}.
An alternative, more concise, notation for \eqref{eq:x}
involves the \emph{complex amplitudes} and \emph{signal poles} associated with
the \ac{FID}:
\begin{subequations}
    \begin{gather}
        \xnonenD(\bth) = \sumM \alpha_m \prodD {z^{(d)}_m}^{\nd},\\
        \alpha_m = \amexpphim,\\
        z_m^{(d)} = \exp\left(
            \left(2 \pi \iu \left(f_m^{(d)} - \foffd\right) - \eta^{(d)}_m\right) \Dtd
        \right).
    \end{gather}
    \label{eq:x-alpha-z}
\end{subequations}

Due to the assumed \ac{AWGN} nature of the noise array, the \ac{pdf} of an
individual noise component is
\begin{equation}
    p(\wnonenD) =
        \frac{1}{2\pi \sigma^2}
        \exp\left( -\frac{\left\lvert \wnonenD \right\rvert^2}{2\sigma^2}\right).
\end{equation}
As the elements are independent and identically distributed, the joint \ac{pdf}
describing the entire noise array is given by the product of each element's
\ac{pdf}:
\begin{equation}
    \begin{split}
        p\left(\bW\right) &=
            \prod_{\none=0}^{\None - 1}
            \cdots
            \prod_{\nD=0}^{\ND - 1}
            \frac{1}{2\pi \sigma^2}
            \exp\left(
                -\frac
                {\left\lvert \wnonenD \right\rvert^2}
                {2\sigma^2}\right) \\
            &= \frac{1}{\left(2\pi \sigma^2\right)^{\mathfrak{N}}}
            \exp\left( -\frac{\left\lVert \bW \right\rVert^2}{2\sigma^2}\right)
    \end{split}
\end{equation}
As the noise array is the difference between the data and model, the
likelihood function of $\bth$ given $\bY$, $\mathcal{L}\left(\bth \vert
\bY\right)$, is given by
\begin{equation}
    \mathcal{L}\left(\bth \vert \bY\right) =
    \frac{1}{\left(2\pi \sigma^2\right)^{\mathfrak{N}}}
        \exp\left( -\frac{\left\lVert \bY - \bX(\bth) \right\rVert^2}{2\sigma^2}\right).
\end{equation}
It is common to consider instead the log-likelihood function of $\bth$ given
$\bY$, $\ell\left(\bth \vert \bY\right)$. As application of the logarithm is a
monotonic transformation, the arguments of the maxima of $\mathcal{L}$ and
$\ell$ are equivalent.
\begin{equation}
    \ell\left(\bth \vert \bY\right) =
        -\mathfrak{N} \ln\left(2 \pi \sigma^2\right)
        -\frac{\left\lVert \bY - \bX(\bth) \right\rVert^2}{2\sigma^2}.
    \label{eq:log-likeihood}
\end{equation}
Equation \ref{eq:log-likeihood} implies that the optimal set of parameters
$\bth^{(*)}$, often referred to as the \acfi{MLE}\acused{MLE},
is that which minimises the
(square) norm of the difference between the data and model
\begin{equation}
    \bthstar = \argmax_{\bth \in \mathbb{R}^{2(D+2)M}}
        \ell\left(\bth \vert \bY\right) \equiv
        \argmin_{\bth \in \mathbb{R}^{2(D+2)M}} \left\lVert \bY - \bX(\bth) \right\rVert^2.
    \label{eq:argmin_y-x}
\end{equation}
The application of \acfi{NLP} is a well-established approach to solve such a
problem\cite{Fletcher1987,Nocedal2006}. The basic principle behind \ac{NLP} is
to iteratively explore, in a methodical way, how a function varies with its
arguments. By using information about the function and optionally its
derivatives, such a routine attempts to find a minimum in the function, and
terminates once this has been achieved. While derivative-free approaches to
\ac{NLP} do exist,\note{(cite examples like simplex, simmulated annealing,
BOBYQA?)} in scenarios where the function under consideration has well-defined,
computationally tractable derivatives, the use of these can be valuable to
solving optimisation problems. The problem outlined in Equation
\ref{eq:argmin_y-x} is such an example.

In order for \ac{NLP} to perform effectively, a large amount of \textit{a
priori} information is typically required, in the form of an initial guess
$\bthzero$. There is precedent for using \ac{NLP} in the context of \ac{NMR}
data estimation, in which substantial user input defines the initial guess (see
the description of \ac{VARPRO} and \ac{AMARES} in the previous chapter). As an
alternative, a routine is presented here in which an initial guess is determined
using \iac{SVD}-based technique requiring no or minimal user input.

\note{NLP acts in a ``correcting'' manner, rather than doing all the work with
no initial information}

\note{Describe what is to come in this chapter...}

\begin{remark}
    In contexts where \ac{1D} datasets are considered specifically, the
    redundant dimension index $^{(1)}$ will be neglected.
\end{remark}
