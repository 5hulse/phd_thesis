\section{Non-Linear Programming}
\label{sec:nlp}
In the devised estimation routine, the result generated using the \ac{MPM},
with optional model order selection, is subsequently subjected to an \ac{NLP}
routine to yield the final parameter estimate. In this section, the theory
behind the \ac{NLP} routine is provided, along with motivation for why it
is employed.

\subsection{An Overview of \ac{NLP}}
\label{subsec:nlp-overview}
In an optimisation problem, the goal is to determine the minimum\footnote{
    In certain applications, the interest could actually be to find the maximum of
    a function. However, it is trivial to transform a maximisation problem into
    a minimisation problem by finding the minimum of the negative of the fidelity.
}
of a function $\Fth: \mathbb{R}^n \rightarrow \mathbb{R}, n \in \mathbb{N}$, often
called the \emph{cost function} or \emph{fidelity}.
This is typically with the goal of determining the argument $\bthstar$ at
which the minimum is found:
\begin{equation}
    \bthstar = \argmin_{\bth \in \mathbb{R}^n} \Fth.
    \label{eq:minF}
\end{equation}
The above problem is \emph{unconstrained}, as there are no limitations that the
parameter vector is subjected to. Unless $\Fth$ has particular properties, such
as convexity\footnote{
    A convex function is one such that a line segment through any two points of
    the function lies above it.
}, it is generally only possible to determine a \emph{local} minimum,
rather than a \emph{global} minimum for high-dimensionality problems such as
the one of interest here. $\bthstar$ is a local
minimiser of $\Fth$ if there is a neighbourhood $V \ni \bthstar$ for which
\begin{equation}
    \Fthstar \leq \Fth\ \forall \bth \in V.
  \label{def:local-minimiser}
\end{equation}
$V \subset \mathbb{R}^n$ is a continuous space such that it is possible to move
a finite amount in any direction away from $\bthstar$ and still be in $V$.
Key to \ac{NLP} are the \emph{necessary conditions}, which define whether a
given vector $\bth$ is a local minimum of the fidelity:
\begin{itemize}
    \item The \emph{first necessary condition} states
        that if $\Fth$ is continuously differentiable, and $\bthstar$ is a local extremum\footnote{
            ``Extremum'' is used here instead of ``minimum'', as the first necessary
            condition applies to maxima of a function as well as minima.
        } of $\Fth$, then the gradient vector $\bdgthstar \coloneq \nabla \Fthstar$ is the
        zero vector:
        \begin{equation}
            \bdgthstar = \symbf{0} \in \mathbb{R}^n
        \end{equation}
    \item The \emph{second necessary condition} subsequently states that
        if $\Fth$ and $\bdgth$ are continuously differentiable, and $\bthstar$ is a
        local minimiser of $\Fth$, then the Hessian matrix $\bdHthstar \coloneq
        \nabla^2 \Fthstar$ is positive semidefinite, i.e.
        \begin{equation}
          \symbf{v}^{\mathrm{T}} \bdHthstar \symbf{v} \geq 0\ \forall \symbf{v} \in \mathbb{R}^n.
        \end{equation}
        \item Furthermore, $\bthstar$ is a \emph{unique} local minimiser if the \emph{second-order
            sufficient condition} is also satisfied, i.e. that the Hessian is positive
            definite:
            \begin{equation}
                \symbf{v}^{\mathrm{T}} \bdHthstar \symbf{v} > 0\ \forall \symbf{v} \in \mathbb{R}^n.
            \end{equation}
\end{itemize}

A plethora of approaches have been established to determine local minima of
scalar functions. One of the better-known strategies is \emph{Newton's method},
in which a quadratic approximation of the fidelity is considered.
For a given iteration $k \in \mathbb{N}_0$, the fidelity is approximated using
\begin{equation}
    \FQth =
        \Fthk +
        \symbf{h}\T \bdgthk +
        \tfrac{1}{2} \symbf{h}^{\mathrm{T}} \bdHthk \symbf{h},
    \label{eq:quad-approx}
\end{equation}
where $\symbf{h} = \bth - \bthk$.  An updated prediction of the parameter
vector is derived by finding the minimum of this quadratic approximation:
\begin{gather}
    \frac{\partial \Fth}{\partial \symbf{h}} =
        \bdgthk + \bdHthk \symbf{h} \notag\\
    \implies 0 = \bdgthk + \bdHthk \left(\bthkplusone - \bthk\right) \notag\\
    \therefore\ \bthkplusone =
        \bthk - \bdHthk^{-1}
        \bdgthk.\label{eq:newton-update}
\end{gather}
This process is repeated, until the convergence criterion as been met:
\begin{equation}
    \left\lVert \bdgthk \right\rVert \leq \epsilon.
\end{equation}
The convergence threshold $\epsilon > 0$ can be tuned based on the desired
accuracy of the result.
\Cref{eq:newton-update} tends not to be used as the update formula in real
optimisation problems; one of the major downsides of the Newton update is the
possibility that it does not lead to a decrease in the fidelity if the Hessian
is not positive definite. Two primary strategies have emerged which are
typically used instead:
\begin{itemize}
    \item \emph{Line search methods}\cite[Chapter 3]{Nocedal2006} determine an
        appropriate direction $\symbf{p}^{(k)}$ along which the updated
        parameter vector is sourced.  After this, an appropriate step length
        $\alpha^{(k)}$ is determined\,---\,typically in an efficient, though not
        optimal manner\,---\,leading to $\bthkplusone = \bthk - \alpha^{(k)}\symbf{p}^{(k)}$.
    \item \emph{Trust region methods}\cite[Chapter 4]{Nocedal2006} define a
        radius $\Updelta^{(k)} > 0$, and determine the minimum of
        \cref{eq:quad-approx} subject to the constraint that
        $\left\lVert\symbf{h}\right\rVert \leq \Updelta^{(k)}$.
\end{itemize}


\begin{algorithm}
    \caption[
        The \acl{NLP} routine employed in this work.
    ]
    {
        The \acs{NLP} routine employed in this work. This makes use of
        Algorithms 4.1 \& 7.2 in \cite{Nocedal2006}, with a extra check
        inserted to deal with any negative-amplitude oscillators which may be
        generated as the routine evolves
        (\crefrange{state:neg-amp-start}{state:neg-amp-end}).
    }
    \label{alg:nlp}
    \begin{algorithmic}[1]
        \Procedure {NLP}{$\bY \in \mathbb{C}^{\None \times \cdots \times \ND}, \bthzero \in \mathbb{R}^{2(D + 1)M}$}
            \State $\trustradius{0} \gets \nicefrac{1}{10} \left\lVert \bdgthzeroY \right\rVert$;
            \State $\trmax \gets 16 \trustradius{0}$;
            \For {$k = 0, 1, \cdots $}
            \State $\symbf{p}^{(k)} \gets \textsc{SteihaugToint}\left(\symbf{Y}, \symbf{\theta}^{(k)}, \trustradius{k}\right)$;
                \Comment{See \cref{alg:steihaug-toint}}
                \State $\rho^{(k)} \gets
                    \frac
                        {\Fphithk - \Fphithkpk}
                        {\FphiQthk - \FphiQthkpk}$;
                \If {$\rho_k < \nicefrac{1}{4}$}
                \label{state:decrease-tr-start}
                \State $\trustradius{k+1} \gets \nicefrac{1}{4} \trustradius{k}$;
                    \label{state:decrease-tr-end}
                    \ElsIf {$\rho_k > \nicefrac{3}{4}$ \textbf{ and } $\left\lVert \symbf{p}^{(k)} \right\rVert = \trustradius{k}$}
                \label{state:increase-tr-start}
                \State $\trustradius{k+1} \gets \min\left(2 \trustradius{k}, \trmax\right)$;
                    \label{state:increase-tr-end}
                \Else
                \State $\trustradius{k+1} \gets \trustradius{k}$;
                \EndIf
                \If{$\rho^{(k)} > \nicefrac{3}{20}$}
                \label{state:large-rho-start}
                    \State $\bthkplusone \gets \bthk + \symbf{p}^{(k)}$;
                    \label{state:large-rho-end}
                \Else
                \label{state:small-rho-start}
                    \State $\symbf{\theta}^{(k+1)} \gets \symbf{\theta}^{(k)}$;
                    \label{state:small-rho-end}
                \EndIf
                \If{$k \bmod 25 = 0 \textbf{ and } \symbf{\theta}^{(k+1)}$ contains negative amplitudes}\label{state:neg-amp-start}
                    \State $\symbf{\theta}^{(0)} \gets \symbf{\theta}^{(k+1)}$ with negative-amplitude oscillators removed;
                    \State $\symbf{\theta}^{(*)}, \symbf{\epsilon}^{(*)} \gets \operatorname{NLP}\left(\symbf{Y}, \symbf{\theta}^{(0)}\right)$;
                \EndIf\label{state:neg-amp-end}
                \If{$\left\lVert \bdgthkplusone \right\rVert < \num[print-unity-mantissa=false]{1e-8}$}
                    \State \textbf{break};
                \EndIf
            \EndFor
            \State $\symbf{\theta}^{(*)} \gets \symbf{\theta}^{(k+1)}$
            \State $\symbf{\epsilon}^{(*)} \gets
                \sqrt{
                    \frac
                    {
                        \Fthstar \diag \left(
                            \left[\bdHthstar\right]^{-1}
                        \right)
                    }
                    {(\None \cdots \ND) - 1}
                }$
            \State \textbf{return} $\symbf{\theta}^{(*)}, \symbf{\epsilon}^{(*)}$;
        \EndProcedure
    \end{algorithmic}
\end{algorithm}
A trust region method is applied in this work,
with the typical steps involved given in
\cref{alg:nlp} (ignoring \crefrange{state:neg-amp-start}{state:neg-amp-end}, which is a custom addition,
see \cref{subsec:phase-variance}). An initial radius for the trust region
$\trustradius{0}$ is defined, along with a maximum permitted radius
$\trmax$, to ensure that excessively adventurous steps do not take place.
For each iteration, a solution to the following sub-problem is sought:
\begin{equation}
    \begin{split}
        \bpk = \argmin_{\bp \in \mathbb{R}^{n}}
            \Fthk +
            (\bthk + \bp)\T \bdgthk +
            \tfrac{1}{2} (\bthk + \bp)\T \bdHthk (\bthk + \bp) \\
        \text{subject to } \left \lVert \bp \right \rVert \leq \trustradius{k}.
    \end{split}
\end{equation}
This sub-problem is not usually solved exactly, but instead an efficient
means of determining a sufficiently good update is used.
Common approaches include computing the Cauchy point, the Dogleg
method, and a truncated conjugate-gradient
approach commonly called the \ac{ST} method\cite[Chapter 7]{Nocedal2006}.
The latter is employed in this work (see \cref{alg:steihaug-toint} and
\cref{lst:tr}). In the \ac{ST} approach, iterates of the conjugate-gradient
method\cite[Chapter 5]{Nocedal2006} are computed, either until an iterate which
is outside the trust region is computed, or negative curvature is discovered.
Once a provisional update $\bthkplusone = \bthk + \bpk$ is determined using the
\ac{ST} method, a metric is considered which indicates how well the
quadratic estimate agrees with the true value of the fidelity:
\begin{equation}
    \rho = \frac
        {\Fthk - \Fthkpk}
        {\FQthk - \FQthkpk}.
\end{equation}
$\rho$ is the ratio between the actual reduction of the fidelity caused
by taking the proposed step, and the predicted reduction based on the quadratic
model. If $\rho$ is sufficiently large, the quadratic model being
used to generate new iterate is deemed to be acting well enough to warrant
accepting the proposed update
(\cref{state:large-rho-start,state:large-rho-end}).
Furthermore, if $\rho$ is particularly close to $1$ and the proposed
update is at the boundary of the trust radius, it is appropriate to enlarge the
radius of the trust region for the next iteration in order to increase the
rate of convergence
(\cref{state:increase-tr-start,state:increase-tr-end}).
On the other hand, a small value of $\rho$ implies that the
quadratic model reflects the true fidelity poorly, such that the proposed
update should be rejected
(\cref{state:small-rho-start,state:small-rho-end}).
As well as this, the trust region's radius should be
decreased such that the model is more likely to behave faithfully
(\cref{state:decrease-tr-start,state:decrease-tr-end}). In general, the
thresholds which dictate whether to accept an update, and whether to adjust the
trust region radius are customisable. The hard-coded numerical values found in
\cref{alg:nlp} are the values used for the results presented in this work.

For the specific case of \ac{FID} estimation using \ac{NLP}, consider a general
$D$-dimensional dataset. As established in \cref{sec:theory-outline}, the
fidelity $\FthY : \mathbb{C}^{\None \times \cdots \times \ND} \times
\mathbb{R}^{2(1 + D)M} \rightarrow \mathbb{R}$ is given by
\begin{equation}
    \FthY = \left \lVert \bY - \bXth \right \rVert^2.
    \label{eq:fidelity}
\end{equation}
The elements of the gradient vector $\bdgthY \in \mathbb{R}^{2(1+D)M}$ and
the Hessian matrix $\bdHthY \in \mathbb{R}^{2(1+D)M \times 2(1+D)M}$ are
derived by taking the first and second partial derivatives of the fidelity with
respect to the elements in $\bth$:
\begin{subequations}
    \begin{gather}
        g_i = -2 \Re
                \left\langle
                    \left(\bY - \bX\right),
                    \frac{\partial \bX}{\partial \theta_i}
                \right\rangle,
        \label{eq:grad} \\
        h_{i,j} = 2 \Re
            \biggl(
                \underbrace{
                    \left\langle
                        \frac{\partial \bX}{\partial \theta_i},
                        \frac{\partial \bX}{\partial \theta_j}
                    \right\rangle
                }_{\circled{1}}
                -
                \underbrace{
                    \left\langle
                        \left(\bY - \bX\right),
                        \frac{\partial^2 \bX}{\partial \theta_i \partial \theta_j}
                    \right\rangle
                }_{\circled{2}}
            \biggl),
            \label{eq:hess}\\
        \forall i,j \in \lbrace 1, \cdots, 2(1+D)M \rbrace.\notag
    \end{gather}
    \label{eq:fidelity-grad-hess}%
\end{subequations}
Computing these requires the generation of a large number of first and second
partial derivatives; the complete set of these are given in
\cref{sec:derivatives}, and a \Python implementation to compute them is given
in \cref{lst:obj-grad-hess}.
Fortunately, fewer second partial derivatives need to be computed than one
might initially suspect; $4(1 + D)^2 M^2$ of these exist per datapoint.
However, all of those involving parameters associated with different
oscillators are zero. The second derivatives are also symmetric, i.e.
\[
    \frac{\partial^2 x_{\nonedotsnD}}{\partial \theta_i \partial \theta_j} \equiv
    \frac{\partial^2 x_{\nonedotsnD}}{\partial \theta_j \partial \theta_i},
\]
which further reduces the number of explicit computations that are necessary.
Finally, the derivatives satisfying $\theta_i = \theta_j = a_m$ are always
zero as well. The upshot of these features is that the total number of second
derivatives that need to be computed is reduced to $(2D^2 + 5D + 2)M$ per
datapoint (see \cref{tab:number-of-derivatives}). This is still rather large,
especially when $D>1$, as will be evidenced in \cref{sec:profiling}.
\begin{table}
    \begin{center}
        \begin{tabular}{ c c c }
            \toprule
            Dimensions &
                \raisebox{\depth}{\#} 1\textsuperscript{st} derivatives &
                \raisebox{\depth}{\#} 2\textsuperscript{nd} derivatives\\
            \midrule
            $1$ & $4M\None$ & $9M\None$\\
            $2$ & $6M\None\Ntwo$ & $20M\None\Ntwo$\\
            $3$ & $8M\None\Ntwo\Nthree$ & $35M\None\Ntwo\Nthree$\\
            $D$ &  $2(1 + D)M N_{\text{tot}}$ &  $(2D^2 + 5D + 2) M N_{\text{tot}}$\\
            \bottomrule
        \end{tabular}
    \end{center}
    \caption{
        The number of first and second derivatives that are necessary to
        compute the gradient vector and Hessian matrix of the fidelity for
        1- 2- and 3-dimensional datasets, as well as a general $D$-dimensional
        dataset.
    }
    \label{tab:number-of-derivatives}
\end{table}

\subsection{Approximating the Hessian}
\label{subsec:hess-approx}
Despite many of the model second derivatives being zero, computation of those
that are not zero, and subsequently using these the form the Hessian matrix,
is an expensive part of the optimisation routine.
Numerous optimisation problems exist where this is the case; as such,
there is considerable precedent for improving the efficiency of optimisation
algorithms by generating approximations of the Hessian which are less demanding
to compute.
Examples include the \ac{GN} method and \ac{LM} algorithm,
which are specifically for \ac{RSS} problems\cite[Chapter
10]{Nocedal2006}, as well as quasi-Newton methods such as the \ac{BFGS}
method\cite[Chapter 6]{Nocedal2006}.

The \ac{GN} and \ac{LM} approaches replace the true Hessian matrix at each
iteration with the following expression:
\begin{equation}
    h_{i,j} = 2 \Re
        \left\langle
            \frac{\partial \bX}{\partial \theta_i},
            \frac{\partial \bX}{\partial \theta_j}
        \right\rangle,
    \label{eq:hess-approx}
\end{equation}
i.e. term \circled{2} in \cref{eq:hess}, which involves the model second
derivatives, is neglected. All that needs to be generated is the \emph{Jacobian}
$\nicefrac{\partial \bX}{\partial \bth}$. This can
bring a very large reduction in the computational cost; no extra
derivatives need to be computed for the Hessian at all since the Jacobian is
already required to generate the gradient vector.
In situations where the residuals between the data and model are small, term
\circled{1} will tend to dominate term \circled{2}, and as such these methods
often enjoy a convergence rate which is comparable to that of Newton's method
when close to a local minimum. Despite this, by invoking this approximation, the
rate of convergence (i.e. the number of iterations required to reach
$\bthstar$) tends to be adversely affected, as is highlighted by the following
example.

\subsection{Visualisation of a Simple Example}
\label{subsec:optim-vis}
\begin{figure}
    \centering
    \includegraphics{optimisation_visualisation/optimisation_visualisation.pdf}
    \caption[
        A visualisation of the trajectory of a 2-parameter optimisation
        involving a simulated \acs{FID}.
    ]
    {
        A visualisation of the trajectory of a 2-parameter optimisation
        involving a simulated \acs{FID} comprising a single signal.
        \textbf{a.} and \textbf{b.} Representations of the signal in
        the time domain and Fourier domain, respectively.
        Black dots: the \ac{FID} to be estimated.
        Solid grey line: the model generated
        using the initial guess $\bthzero$.
        Dotted grey line: the model generated using the optimised result
        $\bthstar$.
        \textbf{c.} A contour plot of the fidelity.
        Blue line: the trajectory of the parameter vector with the true
        Hessian matrix used in computing each update.
        Red line: the analogous trajectory using the Hessian approximation
        in place of the true Hessian.
    }
    \label{fig:optim-vis}
\end{figure}
\Cref{fig:optim-vis} provides a visual example of the application of \ac{NLP}
to estimate a simulated \ac{1D} \ac{FID} comprising a single signal.
The FID was constructed using \cref{eq:general-fid} with $M=1$,
$N = 64$, $\fsw = \qty{5.2}{\hertz}$ ($\Dt \approx
\qty{0.192}{\second}$), and $\foff = \qty{0}{\hertz}$.
It was constructed using the parameters $\bth \in \mathbb{R}^4$ comprising $a=1$,
$\phi=\qty{0}{\radian}$, $f=\qty{1}{\hertz}$, and $\eta=\qty{0.2}{\per\second}$.
\ac{AWGN} was added to the \ac{FID} to give it a target \iac{SNR} of
\qty{10}{\deci\bel}. As the visualisation of 5D space is beyond the scope of
this work, only two parameters, the frequency and damping factor, were optimised
from an initial guess $\bthzero$; the amplitude and phase were fixed to
their true values throughout. The initial guess comprised a frequency of
\qty{1.1}{\hertz}, and a damping factor of \qty{0.8}{\per\second}, with the
solid grey lines in Figures \ref{fig:optim-vis}.a and \ref{fig:optim-vis}.b
denoting the model generated in the time- and
Fourier-domains, respectively. $\bthzero$ was subjected to \ac{NLP} twice. In
the first instance, the exact Hessian matrix given by \cref{eq:hess} was used
in order to compute each update step, while in the second the Hessian
approximation given by \cref{eq:hess-approx} was used. The initial
radius of the trust region was set to $\nicefrac{1}{10}$ of the gradient norm
($\approx 0.3$), which has a precedent in the literature\cite{Gould2005}. The
trajectories of the parameter vector are denoted by blue and red lines in
\cref{fig:optim-vis}.c.
In both cases, the \ac{NLP} routine successfully converged at a result $\bthstar$
in agreement with the true frequency and damping factor used to construct the
\ac{FID}. However, it is clear that using the true Hessian matrix (blue trajectory)
led to a far better rate of convergence compared with the
approximated analogue (red trajectory), which exhibited behaviour commonly
referred to as ``zig-zagging''; this is often seen in gradient
descent methods, in which each update occurs along the opposite direction to
the gradient, and leads to the minimum being reached in an inefficient manner.
14 iterations were required to achieve convergence with the
criterion $\epsilon = \num[print-unity-mantissa=false]{1e-8}$
when the true Hessian was used, while 81 were required for the approximated
case. Despite being an anecdotal example, this highlights that use of the true
Hessian matrix tends to allow a better rate of convergence. However, for
\acp{FID} comprising many signals and far more points, the approximated form
often requires a shorter time to converge overall, especially for \ac{2D}
\acp{FID}, as will be illustrated in \cref{sec:profiling}.

\subsection{Phase Variance Minimisation}
\label{subsec:phase-variance}
\Iac{NLP} procedure tasked with minimising the discrepancy between the model
$\bX$ and the observed data $\bY$ is well-suited to produce an accurate
holistic
representation of the data, assuming a sufficiently large
model order is used. However, as has already been discussed, this does not
necessarily lead to a satisfactory result due to the ill-posed nature of the
estimation problem;
it is desirable to produce an estimate which not only achieves a good fit to
the data in \iac{RSS} sense, but which is also in agreement with the process
underpinning the observation. It is for this reason that iterative procedures
typically require significant quantities of prior knowledge, beyond basic
assumptions of the underlying model, in order to produce meaningful estimation
results.  This is also why they are often able to produce results which agree
better with a spectroscopist's conception of what the ``correct'' parameter
estimate should look like, relative to other methods where such detailed
information is not exploited.

While the \ac{MPM} is often able to generate seemingly reasonable parameter
estimates,
one particular feature of these has been noticed on many occasions;
oscillators in the result often exhibit ``spurious phase behaviour''.
As has been discussed (see the description of phase correction,
\cref{subsec:nmr-proc}),
\ac{NMR} datasets for most experiments comprise signals whose phases
depend on their frequencies to first order. This is routinely
corrected in conventional \ac{NMR} spectral processing, such that all signals
are adjusted to acquire a phase of \qty{0}{\radian}. This feature
can be exploited in order to overcome the aforementioned shortcoming of
the \ac{MPM}, through appropriate regularisation of the \ac{NLP} routine.
Assuming that the data has been phase corrected\footnote{
    Rather than rely on the data being phase-corrected, one could envisage
    replacing the phase variance with a term which guides the
    oscillators to adopt a first-order phase relationship during \ac{NLP}, thus
    removing the need to apply phase correction in the first place. The
    reason why the
    phase variance has been chosen is two-fold. (a) Applying phase-correction
    to \ac{NMR} data is straightforward and can be automated,
    meaning the user would experience minimal burden. (b) As will be discussed
    in \cref{sec:filtering}, it is beneficial to have a spectrum comprising
    pure absorption-mode Lorentzians in order to produce frequency-filtered
    ``sub-\acp{FID}'' from the original data, so the data being estimated will
    be phase-corrected anyway.
}, incorporating the variance of oscillator
phases into the fidelity can lead to improved estimation results; examples of
this will be provided later (\cref{sec:evaluation}).
The updated fidelity becomes
\begin{equation}
    \FphithY = \left \lVert \bY - \bXth \right \rVert^2 + \circvar,
    \label{eq:fidelity-phasevar}
\end{equation}
where $\circvar$ is the \emph{circular variance} of the oscillator phases.
Oscillator phases are an example of a circular variable, as all
phases are wrapped within an interval of size 2$\pi$\,\unit{\radian}. Given an
unwrapped phase $\widetilde{\phi} \in \mathbb{R}$, the
corresponding wrapped phase $\phi \in \left( -\pi, \pi \right]$ is given by
\begin{equation}
    \phi = (\widetilde{\phi} + \pi) \bmod 2 \pi - \pi.
    \label{eq:phase_wrap}
\end{equation}
This makes the conventional (linear)
definition of variance, given by
\begin{subequations}
    \begin{gather}
        \Var_{\shortmid}\hspace*{-3pt}\left(\symbf{\phi}\right) =
            \frac{1}{M} \sum_{m=1}^{M} \left(\phi_m - \mu\left(\symbf{\phi}\right)\right)^2, \\
        \mu\left(\symbf{\phi}\right) = \frac{1}{M} \sum_{m} \phi_m,
    \end{gather}
\end{subequations}
unsuitable as a metric to define the variation in phases. Consider as a simple
example a scenario
where there are two oscillators with phases $\widetilde{\bdphi} = \left[ \pi +
\delta\:\:\pi - \delta \right]\T$ for some small $\delta$.
The phase variance is expected to be small as the phases are similar.
However, with the inclusion of wrapping through application of
\cref{eq:phase_wrap}, these phases would actually be set to $\bdphi = \left[
    -\pi
+ \delta\:\:\pi - \delta \right]\T$, and the linear phase
variance would be large. A definition of variance which accounts for the
periodicity of the phases is needed; the circular variance is defined
as\cite[Chapter 3]{Fisher1993}
\begin{subequations}
    \begin{gather}
        [0, 1] \ni \circvar = 1 - \frac{R}{M},\\
        R = \sqrt{c_{\Sigma}^2 + s_{\Sigma}^2}, \\
        c_{\Sigma} = \sum_{m=1}^M \cos \phi_m, \\
        s_{\Sigma} = \sum_{m=1}^M \sin \phi_m.
    \end{gather}
\end{subequations}
$R$ is the length of the resultant vector produced by summing $M$ unit vectors
with angles given by $\bdphi$. In the case that all the vectors have the
same angle, $R=M$, leading to the variance being $0$ as expected. At the other
extreme, with $M$ vectors uniformly separated about the unit circle\,---\,this leads to there being an angle of $\nicefrac{2 \pi}{M - 1}$\,\unit{\radian} between all
pairs of adjacent vectors\,---\,the
vectors will perfectly cancel, leading to $R=0$. In this case, the maximum
possible variance ($1$) is obtained.
The inclusion of the phase variance into the fidelity is one of the
motivating reasons for normalising the data prior to estimation. Since
$\circvar$ is constrained to the interval $[0, 1]$, if the data were not
normalised it is likely that $\lVert \bY - \bX \rVert^2$ would dominate
$\circvar$ in \cref{eq:fidelity-phasevar}, such that the influence of the phase
variance would be negligible.

The first and second derivatives of the circular variance are required for
the computation of the gradient vector and Hessian matrix, whose updated forms
are
\begin{subequations}
    \begin{gather}
        g_i = -2 \Re
                \left\langle
                    \left(\bY - \bX\right),
                    \frac{\partial \bX}{\partial \theta_i}
                \right\rangle
                + \frac{\partial \circvar}{\partial \theta_i}, \\
        h_{i,j} = 2 \Re
            \biggl(
                    \left\langle
                        \frac{\partial \bX}{\partial \theta_i},
                        \frac{\partial \bX}{\partial \theta_j}
                    \right\rangle
                -
                \underbrace{
                    \left\langle
                        \left(\bY - \bX\right),
                        \frac{\partial^2 \bX}{\partial \theta_i \partial \theta_j}
                    \right\rangle
                }_{\parbox{6em}{\scriptsize{Neglected if approximation used}}}
            \biggl)
            + \frac{\partial^2 \circvar}{\partial \theta_i \partial \theta_j}.
    \end{gather}
\end{subequations}
The derivatives of the phase variance are given by:
\begin{subequations}
    \begin{gather}
        \frac{\partial \circvar}{\partial \theta_i} =
        \begin{cases}
            \frac{1}{RM}
            \left(
                c_{\Sigma} \sin \phi_{i-M} -
                s_{\Sigma} \cos \phi_{i-M}
            \right) & M \leq i < 2M\\
            0 & \text{otherwise}
        \end{cases}\\
        \frac{\partial^2 \circvar}{\partial \theta_i \partial \theta_j} =
        \begin{cases}
            \begin{split}
                \tfrac{1}{RM}\left[
                    \tfrac{1}{R^2}
                    \left(c_{\Sigma} \sin \phi_{i-M}  - s_{\Sigma} \cos \phi_{i-M} \right)^2 \right. \\
                    \left. + c_{\Sigma} \cos \phi_{i-M} + s_{\Sigma} \sin \phi_{i-M}
                    - 1
                \vphantom{\tfrac{1}{RM}}\right]
            \end{split}
            & M \leq i, j < 2M, i = j\\
            \begin{split}
                \tfrac{1}{RM}\left[
                    \tfrac{1}{R^2}
                    \left(c_{\Sigma} \sin \phi_{i-M} - s_{\Sigma} \cos \phi_{i-M} \right) \right.\\
                    \times \left(c_{\Sigma} \sin \phi_{j-M} - s_{\Sigma} \cos \phi_{j-M} \right) \\
                    \left. - \cos\left( \phi_{i-M} - \phi_{j-M} \right)
                    \vphantom{\tfrac{1}{R^2}}
                \right]
            \end{split}
            & M \leq i, j < 2M, i \neq j\\
            0 & \text{otherwise}
        \end{cases}
    \end{gather}
\end{subequations}

The phase variance-regularised fidelity (\cref{eq:fidelity-phasevar}) is
minimised according the unconstrained \ac{NLP} routine described above. It is
therefore possible for oscillators to acquire parameters which are unrealistic
as the optimiser evolves (see the bulleted list on
\cpageref{pg:param-constraints} for a summary of
the expected ranges that the parameters reside in). With the inclusion of
the variance of oscillator phases, there are situations where oscillators
acquire negative amplitudes during the \ac{NLP} routine. Typically, this occurs
when there are oscillators in the \ac{MPM} result with phases
that are far from \qty{0}{\radian}. By acquiring a negative amplitude and a
phase close to
\qty{0}{\radian} (the expected phase of most oscillators in the parameter set
if phase correction has been applied to the data)
little change to the \ac{RSS} term is made, while
$\circvar$ will be reduced. The presence of such oscillators is
undesirable, as they are spurious in the context of phased data. As a
result, the \ac{NLP} routine periodically checks for negative-amplitude oscillators
(\crefrange{state:neg-amp-start}{state:neg-amp-end} in \cref{alg:nlp}). After a
given number of iterations ($25$ is the value given in \cref{alg:nlp})
if any oscillators have acquired negative
amplitudes, they are removed from the model, and the routine continues with a reduced number
of oscillators. While not infallible, this can help to to get rid of excessive oscillators in the
model which do not correspond to true signals in the data. For example, in
scenarios where an overestimate of model order has occurred, either by the
\ac{MDL} or from human error in inspecting the number of spectral peaks, noise
components will be incorporated into the \ac{MPM} result. These typically have
greater variability in their phases; often, such oscillators gain negative
amplitudes as the \ac{NLP} routine evolves, leading to unwanted signals being
removed and a more parsimonious result being obtained.
