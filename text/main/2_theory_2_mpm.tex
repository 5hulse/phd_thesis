\section{Generating an Initial Guess: Matrix Pencil Method}
\label{sec:mpm}
\note{Brief introductory text. Mention applications w. citations}

\subsection{1D Matrix Pencil Method}
\label{subsec:mpm}

\begin{remark}
    In contexts where \ac{1D} datasets are considered specifically, the
    redundant dimension index $^{(1)}$ will be neglected for conciseness.
\end{remark}
The \acfi{MPM}, developed by Hua and Sarkar\cite{Hua1990,Hua1990b,Hua1991}, provides a
route to extracting the signal poles of a \ac{1D} dataset, based on the
assumption that the number or oscillators $M$ is known.
To motivate how the \ac{MPM} works, first consider a dataset which is devoid of
noise, given by \cref{eq:x-alpha-z} with $D=1$:
\begin{equation}
    x_n(\bth) = \sumM \alpha_m^{\vphantom{n}} z_m^n,\\
\end{equation}
Consider the Hankel matrix $\Hx \in \mathbb{C}^{(N-L) \times (L+1)}$:
\begin{equation}
    \Hx =
    \begin{bmatrix}
        x_0 & x_1 & \cdots & x_L\\
        x_1 & x_2 & \cdots & x_{L+1}\\
        \vdots & \vdots & \ddots & \vdots\\
        x_{N-L-1} & x_{N-L} & \cdots & x_{N-1}
    \end{bmatrix}.
\end{equation}
This matrix comprises windowed segments of the FID, with each row comprising
the segment shifted to the right by one point relative to the row above.
$L \in \mathbb{N}$ is the \emph{pencil parameter}, which dictates the size of
each window. From $\Hx$, two matrices are defined, $\Hxone$ and $\Hxtwo$,
formed by the removal of the last or first column of $\Hx$, respectively:
\begin{subequations}
   \begin{gather}
        \Hxone =
        \begin{bmatrix}
            x_0 & x_1 & \cdots & x_{L-1} \\
            x_{1} & x_{2} & \cdots & x_{L} \\
            \vdots & \vdots & \ddots & \vdots\\
            x_{N-L-1} & x_{N-L} & \cdots & x_{N-2}\
        \end{bmatrix}, \\
        \Hxtwo =
        \begin{bmatrix}
            x_{1} & x_{2} & \cdots & x_{L} \\
            x_{2} & x_{3} & \cdots & x_{L+1} \\
            \vdots & \vdots & \ddots & \vdots\\
            x_{N-L} & x_{N-L+1} & \cdots & x_{N-1}\\
        \end{bmatrix}.
   \end{gather}
\end{subequations}
These matrices can be deconstructed into the following forms involving matrices
containing the $M$ signal poles and complex amplitudes that the data comprises:
\begin{subequations}
   \begin{gather}
       \Hxone = \symbf{Z}_{\text{L}} \symbf{A} \symbf{Z}_{\text{R}},\\
       \Hxtwo = \symbf{Z}_{\text{L}} \symbf{A} \symbf{Z}_{\text{D}} \symbf{Z}_{\text{R}},\\
       \mathbb{C}^{\left(\None - \Lone\right) \times M} \ni
       \symbf{Z}_{\text{L}} =
       \begin{bmatrix}
           \symbf{1} &
           \symbf{z} &
           \symbf{z}^2 &
           \cdots &
           \symbf{z}^{N-L-1}
        \end{bmatrix}\T,\\
        \mathbb{C}^{M \times L} \ni
        \symbf{Z}_{\text{R}} =
           \begin{bmatrix}
               \symbf{1} & \symbf{z} & {\symbf{z}}^{2} & \cdots & {\symbf{z}}^{L-1}
           \end{bmatrix} ,\\
        \mathbb{C}^{M \times M} \ni
        \symbf{Z}_{\text{D}} = \diag\left(\symbf{z}\right), \label{eq:ZD}\\
        \mathbb{C}^{M \times M} \ni
        \symbf{A} = \diag\left(\symbf{\alpha}\right),\label{eq:A}\\
        \symbf{\alpha} =
        \begin{bmatrix}
            \alpha_1 & \alpha_2 & \cdots & \alpha_M
        \end{bmatrix}\T,\\
        \symbf{z} =
        \begin{bmatrix}
            z_1 & z_2 & \cdots & z_M
        \end{bmatrix}\T.
   \end{gather}
    \label{eq:HX-decomp}
\end{subequations}
The \emph{matrix pencil} $\Hxtwo - \lambda\Hxone$, with $\lambda \in
\mathbb{C}$, can therefore be expressed as
\begin{equation}
    \Hxtwo - \lambda \Hxone = \symbf{Z}_{\text{L}} \symbf{A} \left(
        \symbf{Z}_{\text{D}} - \lambda \symbf{I}_M
    \right) \symbf{Z}_{\text{R}},
\end{equation}
where $\symbf{I}_M \in \mathbb{C}^{M \times M}$ is the identity matrix.
Assuming that the following condition is met:
\begin{equation}
    M \leq L \leq N - M,\label{eq:pencil_condition}
\end{equation}
the rank of the matrix pencil will be $M$. \cref{eq:pencil_condition}
must be obeyed to ensure that both the number of rows and columns of the matrix
pencil are at least $M$. Now consider the case when the scalar $\lambda$ is
equal to one of the signal poles i.e.  $\lambda = z_m\ \forall m \in
\lbrace 1, \cdots, M \rbrace$. The element $[\symbf{Z}_{\text{D}} -
\lambda \symbf{I}_M]_{m,m}$ will be set to $0$, which will lead to the
determinant of the matrix pencil being $0$. The eigenvalues of the matrix
pencil are the solution of the so-called \emph{generalised eigenvalue problem},
and are defined as\cite[Section 7.7]{Golub2013}
\begin{equation}
    \symbf{z} = \left\lbrace
        z \in \mathbb{C} : \det\left(\Hxtwo - z \Hxone\right) = 0
    \right\rbrace
\end{equation}
One means of finding the signal poles is by finding the eigenvalues of the
matrix $\Hxone^+ \Hxtwo^{\vphantom{+}}$. Deriving the corresponding complex
amplitudes can then be achieved by solving the set of linear equations
\begin{subequations}
    \begin{gather}
        \symbf{\alpha} = \symbf{Z}^+ \symbf{x},\\
        \symbf{Z} =
        \begin{bmatrix}
            \symbf{1} &
            {\symbf{z}} &
            {\symbf{z}}^2 &
            \cdots &
            {\symbf{z}}^{N-1}
        \end{bmatrix}\T.
    \end{gather}
    \label{eq:comp-amps}
\end{subequations}
Extraction of the amplitudes, phases, frequencies, and damping factors from the
signal poles and complex amplitudes can then take place:
\begin{subequations}
    \begin{gather}
        \symbf{a} = \left \lvert \symbf{\alpha} \right \rvert,\\
        \symbf{\phi} = \arctan \left(\frac{\Im (\symbf{\alpha})}{\Re(\symbf{\alpha})}\right),\\
        \symbf{f} = \frac{\fsw}{2 \pi} \Im\left(\ln \symbf{z} \right) + \foff, \\
        \symbf{\eta} = -\fsw \Re\left(\ln \symbf{z}\right).
    \end{gather}
\end{subequations}

\subsubsection{Noisy data}
The presence of noise in the signal complicates the process of
determining the $M$ signal poles, as $\Hy$\,---\,$\Hx$'s equivalent with
elements replaced by the noisy data $\symbf{y}$\,---\, is likely to be
full-rank, given by $\min(N - L, L + 1)$. To minimise the influence of noise on
the estimated signal poles, it is necessary to
generate a rank-reduced matrix $\Hytilde$. By employing the \ac{EYM}
theorem\cite[Section~2.2]{Golub2013}, an appropriate matrix is can be obtained
through \ac{SVD} (see \cref{subsec:linear-algebra}):
\begin{subequations}
    \begin{gather}
    \Hytilde =
        \symbf{U}_M^{\vphantom{\dagger}}
        \symbf{\Sigma}_M^{\vphantom{\dagger}}
        \symbf{V}_M^{\dagger},\\
    \mathbb{C}^{(\None - \Lone) \times M} \ni
        \symbf{U}_M^{\vphantom{\dagger}} =
        \begin{bmatrix}
            \symbf{u}_1 &
            \symbf{u}_2 &
            \cdots &
            \symbf{u}_M
        \end{bmatrix},\\
    \mathbb{C}^{(\Lone + 1) \times M} \ni
        \symbf{V}_M^{\vphantom{\dagger}} =
        \begin{bmatrix}
            \symbf{v}_1 &
            \symbf{v}_2 &
            \cdots &
            \symbf{v}_M
        \end{bmatrix},\\
    \mathbb{C}^{M \times M} \ni
        \symbf{\Sigma}_M^{\vphantom{\dagger}} =
        \diag \left( \sigma_1, \sigma_2, \cdots, \sigma_M \right).
    \end{gather}
\end{subequations}
$\sigma_m$ is the $m$ \textsuperscript{th} largest singular value of $\Hy$;
$\symbf{u}_m \in \mathbb{C}^{N - L}$ and $\symbf{v}_m \in
\mathbb{C}^{L+1}$ are the corresponding left and right singular vectors,
respectively. The \ac{EYM} proves that $\Hytilde$ is the closest matrix of rank
$M$ to $\Hy$ in a Frobenius norm sense, i.e.
\begin{equation}
    \Hytilde = \argmin_{\symbf{A}:\ \rank(\symbf{A}) = M} \left \lVert \symbf{A} - \Hy \right \rVert.
\end{equation}
With a rank-reduced matrix produced from the noisy matrix, the signal poles can
then be derived from the eigenvalues of $\Hytildeone^+
\Hytildetwo^{\vphantom{+}}$, where $\Hytildeone$ and $\Hytildetwo$ have the
same relation to $\Hytilde$ as  $\Hxone$ and  $\Hxtwo$ do to  $\Hx$. As a
less expensive alternative, the same result can be achieved by
computing the eigenvalues of $\symbf{V}_{M1}^+\symbf{V}_{M2}^{\vphantom{+}}$,
with
\begin{subequations}
    \begin{gather}
        \symbf{V}_{M1} =
        \begin{bmatrix}
            \symbf{v}_1 & \symbf{v}_2 & \cdots & \symbf{v}_{M-1}
        \end{bmatrix},\\
        \symbf{V}_{M2} =
        \begin{bmatrix}
            \symbf{v}_2 & \symbf{v}_3 & \cdots & \symbf{v}_{M}
        \end{bmatrix}.
    \end{gather}
\end{subequations}
\cref{alg:mpm} provides a pseudo-code description of the \ac{MPM}, while
\cref{lst:mpm} outlines a \Python implementation of it. For optimal
results, the pencil parameter should adhere to $\lfloor \nicefrac{N}{3} \rfloor
\leq L \leq \lfloor\nicefrac{2N}{3}\rfloor$\cite{Hua1990}. In this work,
$\lfloor\nicefrac{N}{3}\rfloor$ is always used, primarily since the computational
complexity of the method is at a maximum when $L = \nicefrac{N}{2}$\footnote{
    With $L = \lfloor\nicefrac{N}{2}\rfloor$, the matrix
    $\symbf{H}_{\symbf{Y}}$ is at its most ``square'' i.e. the number of rows
    and columns are at their most similar. Matrices which are more
    square will increase the demands on computing the \ac{SVD}, with a
    complexity $\mathcal{O}(\min(L+1,N-L+1)^2 \times \max(L+1,N-L+1))$.
}

\begin{algorithm}
    \caption[
        The \acl{MPM}, with the optional prediction of model order using the
        \acl{MDL}.
    ]
    {
        The \acs{MPM}, with the optional prediction of model order using the
        \acs{MDL}, if $M$ is set to $0$.
    }\label{alg:mpm}
    \begin{algorithmic}[1]
        \Procedure{MPM}{$\by \in \mathbb{C}^{N}, \fsw \in \mathbb{R}_{>0}, \foff \in \mathbb{R}, M \in \mathbb{N}_0$}
            \State $L \gets \left\lfloor \nicefrac{N}{3} \right\rfloor$;
            \State $\symbf{H}_{\symbf{y}} \gets
                \begin{bmatrix}
                    y_{0} & y_{1} & \cdots & y_{L}\\
                    y_{1} & y_{2} & \cdots & y_{L+1}\\
                    \vdots & \vdots & \ddots & \vdots\\
                    y_{N-L-1} & y_{N-L} & \cdots & y_{N-1}\\
                \end{bmatrix}
            $;
            \State $\symbf{U}, \symbf{\sigma}, \symbf{V}^{\dagger} \gets
            \SVD(\symbf{H}_{\symbf{y}})$;
            \If {$M = 0$}
                \State $M \gets \operatorname{MDL}(\symbf{\sigma}, L, N)$;
            \EndIf
            \State $\symbf{V} \gets \left[\symbf{V}^{\dagger}\right]^{\dagger}$;
            \State $\symbf{V}_M \gets \symbf{V}\left[:, :M\right]$;
            \Comment{Retain first $M$ right singular vectors.}
            \State $
                \symbf{V}_{M1}, \symbf{V}_{M2} \gets
                \symbf{V}_M\left[:,:M-1\right],
                \symbf{V}_M\left[:,1:\right]
            $;
            \Comment{Remove last/first column}
            \State $\symbf{z} \gets \textsc{Eigenvalues}\left(\symbf{V}_{M1}^+ \symbf{V}_{M2}^{\vphantom{+}}\right)$;
            \State $\symbf{Z} \gets
                \begin{bmatrix}
                    \symbf{1} & \symbf{z} & {\symbf{z}}^2 & \cdots & {\symbf{z}}^{N}
                \end{bmatrix}\T
            $;
            \State $\bdalpha \gets \symbf{Z}^+ \bY$;
            \State $
                \symbf{a} \gets \left\lvert\symbf{\alpha}\right\rvert$;
            \State $\symbf{\phi} \gets \arctan
                \left(\frac{\Im(\symbf{\alpha})}{\Re(\symbf{\alpha})}\right)
            $;
            \State $\symbf{f} \gets \frac{\fsw}{2\pi} \Im \left( \ln \symbf{z} \right) + \foff$;
            \State $\symbf{\eta} \gets -\fsw \Re \left( \ln \symbf{z} \right)$;
            \If {$\symbf{\eta}$ contains negative values}
            \Comment{Purge any oscillators with negative damping}
                \State Remove these from $\symbf{\eta}$, and remove the
                corresponding values from
                $\symbf{a}$, $\symbf{\phi}$, and $\symbf{f}$;
            \EndIf
            \State $\bthzero \gets
                \begin{bmatrix}
                    \symbf{a}\T &
                    \symbf{\phi}\T &
                    \symbf{f}\T &
                    \symbf{\eta}\T
                \end{bmatrix}\T
            $;
            \State \textbf{return} $\bthzero$;
        \EndProcedure
        \Statex
        \Procedure{MDL}{$\symbf{\sigma} \in \mathbb{R}^{L+1}, L \in \mathbb{N}, N \in \mathbb{N}$}
            \State $\operatorname{MDL}_{k-1} \gets \infty$
            \Comment{Ensure that on first iteration ($k=0$), the conditional does not hold}
                \For {$k = 0, \cdots, L$}
                    \State $\operatorname{MDL}_k \gets
                    -\ln\left(
                        \frac
                        {\prod_{r=k}^{L-1} \sigma_{r+1}^{\nicefrac{1}{L-k}}}
                        {\frac{1}{L-k} \sum_{r=k}^{L-1} \sigma_{r+1}}
                    \right)^{(L-k)N}$;
                    \If {$\operatorname{MDL}_k > \operatorname{MDL}_{k-1}$}
                        \State $M \gets k-1$;
                        \State \textbf{break};
                        \Else\State$\operatorname{MDL}_{k-1} \gets \operatorname{MDL}_k$;
                    \EndIf
                \EndFor
                \State \textbf{return} $M$;
        \EndProcedure
    \end{algorithmic}
\end{algorithm}


\subsection{2D Matrix Enhancement and Matrix Pencil Method}
\label{subsec:mmempm}
The \ac{MPM} was extended for the consideration of \ac{2D} data by Hua with the
\acfi{MEMPM}\cite{Hua1992}. The method centers around the enhanced matrix $\EY
\in \mathbb{C}^{\left(\Lone \Ltwo\right) \times \left(\None - \Lone +
1\right)\left(\Ntwo - \Ltwo + 1\right)}$, a block Hankel matrix of the form
\begin{subequations}
    \begin{gather}
        \EY =
        \begin{bmatrix}
            \symbf{H}_{\symbf{Y},0} & \symbf{H}_{\symbf{Y},1} & \cdots & \symbf{H}_{\symbf{Y},\None - \Lone} \\
            \symbf{H}_{\symbf{Y},1} & \symbf{H}_{\symbf{Y},2} & \cdots & \symbf{H}_{\symbf{Y},\None - \Lone + 1} \\
            \vdots & \vdots & \ddots & \vdots \\
            \symbf{H}_{\symbf{Y},\Lone - 1} & \symbf{H}_{\symbf{Y},\Lone} & \cdots & \symbf{H}_{\symbf{Y},\None - 1}
        \end{bmatrix}, \\
        \def\arraystretch{1.3}
        \symbf{H}_{\symbf{Y},\none} =
        \begin{bmatrix}
            y_{ \none, 0 } & y_{ \none, 1 } & \cdots & y_{ \none, \Ntwo - \Ltwo } \\
            y_{ \none, 1 } & y_{ \none, 2 } & \cdots & y_{ \none, \Ntwo - \Ltwo + 1 } \\
            \vdots & \vdots & \ddots & \vdots \\
            y_{ \none, \Ltwo - 1 } & y_{ \none, \Ltwo } & \cdots & y_{ \none, \Ntwo - 1 }
        \end{bmatrix}.
    \end{gather}
\end{subequations}
In a similar fashion to \cref{eq:HX-decomp},
$\symbf{H}_{\symbf{X},\none}$, the noiseless equivalent to
$\symbf{H}_{\symbf{Y},\none}$, can be expressed as
\begin{equation}
    \symbf{H}_{\symbf{X},\none} =
        \symbf{Z}^{(2)}_{\text{L}}
        \symbf{A}
        {\symbf{Z}^{(1)}_{\text{D}}}^{\none}
        \symbf{Z}^{(2)}_{\text{R}}.
\end{equation}
This then enables the noiseless enhanced matrix to be expressed as
\begin{subequations}
    \begin{gather}
        \symbf{E}_{\symbf{X}} =
        \symbf{E}_{\text{L}}
        \symbf{A}
        \symbf{E}_{\text{R}},\\
        \mathbb{C}^{\Lone \Ltwo \times M} \ni
        \symbf{E}_{\text{L}} =
        \begin{bmatrix}
            \symbf{Z}^{(2)}_{\text{L}} \\
            \symbf{Z}^{(2)}_{\text{L}} \symbf{Z}^{(1)}_{\text{D}} \\
            \vdots \\
            \symbf{Z}^{(2)}_{\text{L}} {\symbf{Z}^{(1)}_{\text{D}}}^{\Lone - 1} \\
        \end{bmatrix},\label{eq:EL}\\
        \mathbb{C}^{M \times \left(\None - \Lone + 1\right)\left(\Ntwo - \Ltwo + 1\right)} \ni
        \symbf{E}_{\text{R}} =
        \begin{bmatrix}
            \symbf{Z}^{(2)}_{\text{R}} &
            \symbf{Z}^{(1)}_{\text{D}} \symbf{Z}^{(2)}_{\text{R}} &
            \cdots &
            {\symbf{Z}^{(1)}_{\text{D}}}^{\None - \Lone} \symbf{Z}^{(2)}_{\text{R}} \\
        \end{bmatrix}.
    \end{gather}
\end{subequations}
As was the case in the \ac{1D} \ac{MPM}, \ac{SVD} can be utilised to generate a
filtered matrix $\EYtilde$ with its rank reducd to $M$, in accordance witht he
\ac{EYM} theorem:
\begin{equation}
    \EYtilde =
        \symbf{U}_M^{\vphantom{\dagger}}
        \symbf{\Sigma}_M^{\vphantom{\dagger}}
        \symbf{V}_M^{\dagger}
\end{equation}
If the conditions $\Nd - L^{(d)} + 1 \geq M\ \forall d \in \lbrace 1, 2
\rbrace$ are met, $\range\left(\symbf{U}_M\right) =
\range\left(\symbf{E}_{\text{L}}\right)$. This implies that there is some
nonsingular matrix $\symbf{T} \in \mathbb{C}^{M \times M}$ such that
\begin{equation}
    \symbf{U}_M = \symbf{E}_{\text{L}} \symbf{T}.
\end{equation}
Now consider the following two matrices:
\begin{subequations}
    \begin{gather}
        \symbf{U}_{M1} = \symbf{E}^{\vphantom{(1)}}_{\text{L}1} \symbf{T},\\
        \symbf{U}_{M2} = \symbf{E}^{\vphantom{(1)}}_{\text{L}1} \symbf{Z}^{(1)}_{\text{D}} \symbf{T},\\
        \mathbb{C}^{\Lone \left(\Ltwo - 1\right) \times M} \ni
        \symbf{E}_{\text{L}1} =
        \begin{bmatrix}
            \symbf{Z}^{(2)}_{\text{L}} \\
            \symbf{Z}^{(2)}_{\text{L}} \symbf{Z}^{(1)}_{\text{D}} \\
            \vdots \\
            \symbf{Z}^{(2)}_{\text{L}} {\symbf{Z}^{(1)}_{\text{D}}}^{\Lone - 2}
        \end{bmatrix}.
    \end{gather}
\end{subequations}
$\symbf{U}_{M1}$ and $\symbf{U}_{M2}$ correspond the $\symbf{U}_M$ with the
last and first $\Ltwo$ rows removed, respectively. The matrix pencil for
$\symbf{U}_{M1}$ and $\symbf{U}_{M2}$ can be expressed as
\begin{equation}
    \symbf{U}_{M1} - \lambda \symbf{U}_{M2} =
    \symbf{E}_{\text{L}1} \left( \symbf{Z}^{(1)}_{\text{D}} - \lambda \symbf{I}_M \right) \symbf{T}.
\end{equation}
As seen previously, this matrix structure implies that the elements of
$\bdzone$ are the solutions to the generalised eigenvalue problem, such that
they are the eigenvalues of $\symbf{U}_{M1}^{+} \symbf{U}_{M2}^{\vphantom{+}}$.

To extract the signal poles in the other dimension, $\bdztwo$, the permutation
matrix is defined:
\begin{equation}
    \mathbb{R}^{\Lone \Ltwo \times \Lone \Ltwo} \ni
    \symbf{P} =
    \begin{bmatrix}
        \symbf{e}\left(1\right)\T \\
        \symbf{e}\left(\Ltwo + 1\right)\T \\
        \vdots \\
        \symbf{e}\left(1 + \left(\Lone - 1\right)\Ltwo\right)\T \\
        \symbf{e}\left(2\right)\T \\
        \symbf{e}\left(2 + \Ltwo\right)\T \\
        \vdots \\
        \symbf{e}\left(2 + \left(\Lone - 1\right)\Ltwo\right)\T \\
        \vdots \\
        \vdots \\
        \symbf{e}\left(\Ltwo\right)\T \\
        \symbf{e}\left(2\Ltwo\right)\T \\
        \vdots \\
        \symbf{e}\left(\Lone \Ltwo\right)\T \\
    \end{bmatrix}.
\end{equation}
$\symbf{e}\left(i\right) \in \mathbb{R}^{\Lone \Ltwo}$ corresponds to a unit
vector comprising zeros except for $e_i = 1\ \forall i \in \lbrace 1, \cdots,
\Lone\Ltwo \rbrace$.
Multiplying $\symbf{E}_{\text{L}}$ by the permutation matrix leads to a matrix
in which the roles of the two sets of signal poles are effectively swapped:
\begin{equation}
    \symbf{E}_{\text{LP}} \coloneq \symbf{P} \symbf{E}_{\text{L}} =
    \begin{bmatrix}
        \symbf{Z}^{(1)}_{\text{L}} \\
        \symbf{Z}^{(1)}_{\text{L}} \symbf{Z}^{(2)}_{\text{D}} \\
        \vdots \\
        \symbf{Z}^{(1)}_{\text{L}} {\symbf{Z}^{(2)}_{\text{D}}}^{\Ltwo - 1} \\
    \end{bmatrix}.\label{eq:ELP}
\end{equation}
Note the similarity of \cref{eq:ELP} with \cref{eq:EL}, which
implies that with the same reasoning as given above, $\bdztwo$ can be derived
by extracting the eigenvalues of $\symbf{U}_{M\text{P}1}^+
\symbf{U}_{M\text{P}2}^{\vphantom{+}}$, where $\symbf{U}_{M\text{P}1}$ and
$\symbf{U}_{M\text{P}2}$ correspond to $\symbf{P} \symbf{U}_M$
with the last and first $\Lone$ rows removed, respectively.

In the original account on the \ac{MEMPM}, the final stage involved employing a
pairing algorithm in order to assign the uncorrelated signal poles in $\bdzone$
with $\bdztwo$\cite{Hua1992}. The \emph{modified} \ac{MEMPM} (\acs{MMEMPM}) was
developed in order to overcome two issues with the pairing algorithm: (a) it is
computationally expensive (b) it is prone to return incorrect
pairings\cite{Chen2007}.  As well as the eigenvalues of
$\symbf{U}^{+}_{M1}\symbf{U}_{M2}^{\vphantom{+}}$, the \ac{MMEMPM} requires
extraction of the eigenvectors too, which are contained in the matrix
$\symbf{W}^{(1)}$. Assuming that there are no repeated poles in $\bdzone$, the
correctly paired second set of poles is then generated via
\begin{equation}
    \bdztwo = \diag\left(
        \symbf{W}^{-1}
        \symbf{U}_{M\text{P}1}^+
        \symbf{U}_{M\text{P}2}^{\vphantom{+}}
        \symbf{W}
    \right)
\end{equation}
\note{Talk about case of paired eigenvalues.}

See \cref{alg:mmempm} for a pseudo-code outline, and \cref{lst:mmempm} for a
\Python implementation of the \ac{MMEMPM}.

\subsection{Model Order Selection}
\label{subsec:model-order}
The \ac{MPM} and \ac{MMEMPM} operate under the assumption that the model order
$M$ is known, or at least has been predicted.
It is possible that an individual inspecting the \ac{FID}'s spectrum could
predict $M$ based on the number of peaks visible, however subjective means of
predicting model order are typically viewed as disadvantageous as they have bias
associated with them.
There are various non-subjective criteria which have been established for
estimating the model order of a given signal, with probably the two most
prominent being the \ac{AIC}\cite{Akaike1974} and
\ac{MDL}\cite{Schwarz1978,Rissanen1978}. Both of these consider a family of
potential models which describe a given set of observations, parametrised by
the vector $\bth$. For the purpose of \ac{1D} \ac{FID} estimation, the family
of potential models comprise \cref{eq:x}, with variable $M$. Both the \ac{AIC}
and \ac{MDL} take the same general form:
\begin{equation}
    \mathcal{C}(k) = -c \ln \left(\mathcal{L} \left(\bthstar | \by \right)
    \right) + \mathcal{P}(k) \text{ with } \bthstar \in \mathbb{R}^{4k},
\end{equation}
$\forall k \in \lbrace 0, 1, \cdots \rbrace$. $\mathcal{L} \left(\bthstar |
\by \right)$ is the likelihood
function of a given model, with order $k$, at the \ac{MLE}
$\bthstar$, $c \in \mathbb{R}_{>0}$ is a scaling
constant, and $\mathcal{P}$ is a penalising function, which acts to correct
for bias. As the model order increases, the likelihood function at the \ac{MLE}
will increase in size, as a model with more parameters will be able to fit a
given dataset more accurately. However, as the model order increases, there will
become a point where practically all of the deterministic part of the signal
has been incorporated into the model, and increasing the model order further
leads to the model also accounting for noise. The penalising term, which
is larger for higher $k$, is required in order to estimate a model
order which is parsimonious. Wax and Kailath derived an expression for
the likelihood at the \ac{MLE} for models comprising a summation of
complex sinusoids\cite{Wax1985}\footnote{
    The expression in original paper considers the eigenvalues of the
    covariance matrix for the signal, rather than the singular values of $\Hy$.
    These are equivalent however.
}:
\begin{equation}
    \mathcal{L}\left(\bthstar \in \mathbb{R}^{4k} | \by\right) = \left(
        \frac{
            \prod_{r=k}^{L-1} \sigma_{r+1}^{\nicefrac{1}{L-k}}
        }{
            \frac{1}{L-k} \sum_{r=k}^{L-1} \sigma_{r+1}
        }
        \right)^{(L - k) N},
        \label{eq:wax-pdf}
\end{equation}
$\forall k \in \lbrace 0, 1, \cdots, L - 1 \rbrace$. $\symbf{\sigma} \in
\mathbb{R}^{\Lone}$ is the set of singular values of $\symbf{H}_{\symbf{y}}$,
in decreasing order. The forms of the \ac{AIC} and \ac{MDL} are given by
\begin{subequations}
    \begin{gather}
        \operatorname{AIC}(k) = -2 \ln\left( \mathcal{L} \left(\hat{\bth} | \by\right) \right) + 2k(2 L - k), \\
        \operatorname{MDL}(k) = -\ln\left( \mathcal{L} \left(\hat{\bth} | \by\right) \right) + \tfrac{1}{2} k(2 L - k) \ln N. \label{eq:mdl}
    \end{gather}
\end{subequations}
The \ac{AIC} has been shown to be inconsistent in that it tends to overestimate
the model order as the number of samples increases\cite{Wax1985}. For this
reason, the \ac{MDL} has found greater favour in signal processing
applications. As such, by default the estimation routine employed in this work
utilises the \ac{MDL}:
\begin{equation}
    M = \argmin_{k \in \mathbb{N}_0 :\ k < L} \operatorname{MDL} (k).
\end{equation}
\begin{figure}
    \centering
    \includegraphics{mdl/mdl.pdf}
    \caption[
        A visualisation of the behaviour of the \acs{MDL} for three different
        \acsp{FID} comprising the same deterministic component, but with
        different noise variances.
    ]{
        A visualisation of the behaviour of the \acs{MDL} for three different
        \acsp{FID} comprising the same deterministic component ($\bx$) but
        with different noise instances of differing variances. The model used
        to construct the \acp{FID} features 7
        signals. The three \acsp{SNR} used were
        \qty{7}{\deci\bel} (red), \qty{12}{\deci\bel} (blue), and
        \qty{20}{\deci\bel} (yellow). The \acsp{FID} were generated with $N
        = 256$.
        \textbf{a.} Spectra of the three \acsp{FID}.
        \textbf{b.} The values of the 14 most significant singular values
        associated with the Hankel matrix $\Hy$, with the
        pencil parameter $L$ set to $\lfloor \nicefrac{N}{3} \rfloor =
        85$.
        \textbf{c.} Square points with dotted lines: The negative log-likeliood
        at the \ac{MLE}, i.e. the first term of \cref{eq:mdl}.
        Grey line: the penalty component of the \ac{MDL}, given by the second
        term in \cref{eq:mdl}.
        Circular points with solid lines: the \ac{MDL}.
        Stars denote the minimum of the \ac{MDL} for a given \ac{FID}. The
        \qty{20}{\deci\bel}
        signal is correctly deemed to have a model order of 7, while the other
        two are underestimated (predicted models orders are 5 and 3 for the
        \qty{12}{\deci\bel} and \qty{7}{\deci\bel} \acsp{FID}, respectively).
    }
    \label{fig:mdl}
\end{figure}
Applying the \ac{MDL} for model order selection, and subsequently using the
\ac{MPM} for parameter estimation is the basis of the \ac{ITMPM}\cite{Lin1997}.
\Cref{fig:mdl} illustrates the form of the \ac{MDL} for three signals
with equivalent underlying models, with $M=7$, and noise instances with
different variances. The first 14 singular values of $\Hy$
are plotted in panel b, where it can be seen that beyond the first 7,
which account for signal components, the subsequent singular values, decrease
at a far slower rate. The noise subspace for \acp{FID} with higher \acp{SNR}
have singular values which are (a) smaller in magnitude and (b) more
consistent, such that distinguishing the noise and signal subspaces is an
easier task (cf. the yellow and red lines in panel b). As such, the
\ac{MDL} is more likely to provide a faithful estimate of the true number of
components in the \ac{FID} (panel c) when the \ac{SNR} is higher.

\note{Mention that multidimensional criteria are available, though not
implemented in this work.}
