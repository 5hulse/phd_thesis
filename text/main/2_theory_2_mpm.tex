\section{Matrix Pencil Methods}
\label{sec:mpm}
In this section, a description the \ac{MPM}\,---\,also referred to as the
generalized pencil-of-function method~\cite{Hua1989}\,---\,is provided. This
work is limited to the consideration of \ac{1D} and \ac{2D} \ac{NMR} data, and
so both the original \ac{1D} \ac{MPM} and its \ac{2D} analogue, the \ac{MMEMPM}
are discussed.

\subsection{The 1D MPM}
\label{subsec:mpm}
The \ac{MPM} provides a route to extracting the signal poles of a \ac{1D}
dataset, based on the assumption that the number of signals $M$ that the data
comprises is known~\cite{Hua1990,Hua1990b,Hua1991}.
To motivate how the \ac{MPM} works, first consider a dataset which is devoid of
noise, given by \cref{eq:x-alpha-z} with $D=1$:
\begin{equation}
    x_n(\bth) = \sumM \alpha_m^{\vphantom{n}} z_m^n
    \quad \forall n \in \lbrace 0, \cdots, N-1 \rbrace.
\end{equation}
Consider the Hankel matrix $\Hx \in \mathbb{C}^{(N-L) \times (L+1)}$:
\begin{equation}
    \Hx =
    \begin{bmatrix}
        x_0 & x_1 & \cdots & x_L\\
        x_1 & x_2 & \cdots & x_{L+1}\\
        \vdots & \vdots & \ddots & \vdots\\
        x_{N-L-1} & x_{N-L} & \cdots & x_{N-1}
    \end{bmatrix}.
\end{equation}
This matrix comprises windowed segments of the FID, with each row featuring
the segment shifted to the right by one point relative to the row above.
$L \in \mathbb{N}$ is referred to as the \emph{pencil parameter}, which
dictates the size of each window. From $\Hx$, two matrices are defined:
$\Hxone$ and $\Hxtwo$.  These are formed by the removal of the last and first
column of $\Hx$, respectively:
\begin{subequations}
   \begin{gather}
        \Hxone =
        \begin{bmatrix}
            x_0 & x_1 & \cdots & x_{L-1} \\
            x_{1} & x_{2} & \cdots & x_{L} \\
            \vdots & \vdots & \ddots & \vdots\\
            x_{N-L-1} & x_{N-L} & \cdots & x_{N-2}\
        \end{bmatrix}, \\
        \Hxtwo =
        \begin{bmatrix}
            x_{1} & x_{2} & \cdots & x_{L} \\
            x_{2} & x_{3} & \cdots & x_{L+1} \\
            \vdots & \vdots & \ddots & \vdots\\
            x_{N-L} & x_{N-L+1} & \cdots & x_{N-1}\\
        \end{bmatrix}.
   \end{gather}
\end{subequations}
$\Hxone$ and  $\Hxtwo$ can be deconstructed into the following forms involving
matrices containing the $M$ signal poles and complex amplitudes that the data
comprises:
\begin{subequations}
   \begin{gather}
       \Hxone = \symbf{Z}_{\text{L}} \symbf{A} \symbf{Z}_{\text{R}},\\
       \Hxtwo = \symbf{Z}_{\text{L}} \symbf{A} \symbf{Z}_{\text{D}} \symbf{Z}_{\text{R}},\\
       \mathbb{C}^{\left(\None - \Lone\right) \times M} \ni
       \symbf{Z}_{\text{L}} =
       \begin{bmatrix}
           \symbf{1} &
           \symbf{z} &
           \symbf{z}^2 &
           \cdots &
           \symbf{z}^{N-L-1}
        \end{bmatrix}\T,\\
        \mathbb{C}^{M \times L} \ni
        \symbf{Z}_{\text{R}} =
           \begin{bmatrix}
               \symbf{1} & \symbf{z} & {\symbf{z}}^{2} & \cdots & {\symbf{z}}^{L-1}
           \end{bmatrix} ,\\
        \mathbb{C}^{M \times M} \ni
        \symbf{Z}_{\text{D}} = \diag\left(\symbf{z}\right), \label{eq:ZD}\\
        \mathbb{C}^{M \times M} \ni
        \symbf{A} = \diag\left(\symbf{\alpha}\right),\label{eq:A}\\
        \symbf{\alpha} =
        \begin{bmatrix}
            \alpha_1 & \alpha_2 & \cdots & \alpha_M
        \end{bmatrix}\T,\\
        \symbf{z} =
        \begin{bmatrix}
            z_1 & z_2 & \cdots & z_M
        \end{bmatrix}\T.
   \end{gather}%
    \label{eq:HX-decomp}%
\end{subequations}%
The \emph{matrix pencil} $\Hxtwo - \lambda\Hxone$, with $\lambda \in
\mathbb{C}$, can therefore be expressed as
\begin{equation}
    \Hxtwo - \lambda \Hxone = \symbf{Z}_{\text{L}} \symbf{A} \left(
        \symbf{Z}_{\text{D}} - \lambda \symbf{I}_M
    \right) \symbf{Z}_{\text{R}},
\end{equation}
where $\symbf{I}_M \in \mathbb{C}^{M \times M}$ is the identity matrix.
Assuming that the condition $M \leq L \leq N - M$ is met\,---\,this ensures
that both the number of rows and columns of the matrix pencil are at least
$M$\,---\,the rank of the matrix pencil will be $M$.
Now consider the case when the scalar $\lambda$ is
equal to one of the signal poles i.e.  $\lambda = z_m\ \forall m \in
\lbrace 1, \cdots, M \rbrace$: the element $[\symbf{Z}_{\text{D}} -
\lambda \symbf{I}_M]_{m,m}$ becomes $0$, which will lead to the
determinant of the matrix pencil being $0$. The signal poles are therefore the
\emph{generalised eigenvalues} of $\Hxtwo - \lambda \Hxone$~\cite[Section
7.7]{Golub2013}:
\begin{equation}
    \symbf{z} = \left\lbrace
        z \in \mathbb{C} : \det\left(\Hxtwo - z \Hxone\right) = 0
    \right\rbrace
\end{equation}
One means of extracting the signal poles is by computing the eigenvalues of the
matrix $\Hxone^+ \Hxtwo^{\vphantom{+}}$. Deriving the corresponding complex
amplitudes can then be achieved by solving the set of linear equations given by
\cref{eq:complex-amplitudes}, with $\by$ replaced by $\bx$.
Extraction of the amplitudes, phases, frequencies, and damping factors from the
signal poles and complex amplitudes can then take place:%
\begin{subequations}%
    \begin{gather}
        \symbf{a} = \left \lvert \symbf{\alpha} \right \rvert,\\
        \symbf{\phi} = \arctan \left(\frac{\Im (\symbf{\alpha})}{\Re(\symbf{\alpha})}\right),\\
        \symbf{f} = \frac{\fsw}{2 \pi} \Im\left(\ln \symbf{z} \right) + \foff,\label{eq:poles-to-f}\\
        \symbf{\eta} = -\fsw \Re\left(\ln \symbf{z}\right).
    \end{gather}
\end{subequations}
Noise corruption complicates the process of
determining the $M$ signal poles. The Hankel matrix associated with
\iac{FID} corrupted by noise $\Hy$ is likely to be of full-rank, i.e.
$\rank(\Hy) = \min(N - L, L + 1)$. To minimise the influence of noise on
the estimated signal poles, it is necessary to
generate a rank-reduced matrix $\Hytilde$. By employing the \ac{EYM}
theorem~\cite[Section~2.2]{Golub2013}, an appropriate matrix can be obtained
through \ac{SVD}:%
\begin{subequations}%
    \begin{gather}%
    \Hytilde =
        \symbf{U}_M^{\vphantom{\dagger}}
        \symbf{\Sigma}_M^{\vphantom{\dagger}}
        \symbf{V}_M^{\dagger},\\
    \mathbb{C}^{(\None - \Lone) \times M} \ni
        \symbf{U}_M^{\vphantom{\dagger}} =
        \begin{bmatrix}
            \symbf{u}_1 &
            \symbf{u}_2 &
            \cdots &
            \symbf{u}_M
        \end{bmatrix},\\
    \mathbb{C}^{(\Lone + 1) \times M} \ni
        \symbf{V}_M^{\vphantom{\dagger}} =
        \begin{bmatrix}
            \symbf{v}_1 &
            \symbf{v}_2 &
            \cdots &
            \symbf{v}_M
        \end{bmatrix},\\
    \mathbb{C}^{M \times M} \ni
        \symbf{\Sigma}_M^{\vphantom{\dagger}} =
        \diag \left( \sigma_1, \sigma_2, \cdots, \sigma_M \right).
    \end{gather}
\end{subequations}
$\sigma_m$ is the $m$\textsuperscript{th} largest singular value of $\Hy$,
while $\symbf{u}_m \in \mathbb{C}^{N - L}$ and $\symbf{v}_m \in
\mathbb{C}^{L+1}$ are the corresponding left and right singular vectors,
respectively. The \ac{EYM} theorem proves that $\Hytilde$ is the closest matrix
of rank $M$ to $\Hy$ in a Frobenius norm sense, i.e.
\begin{equation}
    \Hytilde = \argmin_{\symbf{A}:\ \rank(\symbf{A}) = M} \left \lVert \symbf{A} - \Hy \right \rVert.
\end{equation}
The intention behind applying the \ac{SVD} in the \ac{MPM} and other
methods like \ac{LPSVD} and \ac{HSVD} is to discard components in the data
matrix which are associated with noise. Assuming the \ac{SNR} of \iac{FID} is
sufficiently high, a clear distinction is often found between the magnitudes of
singular values associated with true signal components and those associated
with noise (see \cref{fig:mdl}.b for an example of this).

The signal poles are finally extracted by computing the eigenvalues of
$\Hytildeone^+ \Hytildetwo^{\vphantom{+}}$, where
$\Hytildeone$ and $\Hytildetwo$ have the same relation to $\Hytilde$ as
$\Hxone$ and  $\Hxtwo$ do to  $\Hx$. As a less expensive alternative, the same
result can be achieved by computing the eigenvalues of
$\symbf{U}_{M1}^+\symbf{U}_{M2}^{\vphantom{+}}$, with
\begin{subequations}
    \begin{gather}
        \symbf{U}_{M1} =
        \begin{bmatrix}
            \symbf{u}_1 & \symbf{u}_2 & \cdots & \symbf{u}_{M-1}
        \end{bmatrix},\\
        \symbf{U}_{M2} =
        \begin{bmatrix}
            \symbf{u}_2 & \symbf{u}_3 & \cdots & \symbf{u}_{M}
        \end{bmatrix}.
    \end{gather}
\end{subequations}
\cref{alg:mpm} provides a pseudo-code description of the \ac{MPM}, while
\cref{lst:mpm} provides an example \Python implementation. For optimal
results, the pencil parameter should adhere to $\lfloor \nicefrac{N}{3} \rfloor
\leq L \leq \lfloor\nicefrac{2N}{3}\rfloor$~\cite{Hua1990}. In this work,
$\lfloor\nicefrac{N}{3}\rfloor$ is always used, primarily since the computational
burden of the method is at a maximum when $L = \nicefrac{N}{2}$\footnote{
    With $L = \lfloor\nicefrac{N}{2}\rfloor$, the matrix
    $\Hy$ is at its most ``square'' i.e. the number of rows
    and columns are at their most similar. Matrices which are more
    square will increase the demands on computing the \ac{SVD}, with
    complexity $\mathcal{O}(\min(L+1,N-L+1)^2 \times \max(L+1,N-L+1))$.
}.

\begin{algorithm}
    \caption[
        The \acl{MPM}, with the optional prediction of model order using the
        \acl{MDL}.
    ]
    {
        The \acs{MPM}, with the optional prediction of model order using the
        \acs{MDL} if $M$ is set to $0$.
    }\label{alg:mpm}
    \begin{algorithmic}[1]
        \Procedure{MPM}{$\by \in \mathbb{C}^{N}, \fsw \in \mathbb{R}_{>0}, \foff \in \mathbb{R}, M \in \mathbb{N}_0$}
            \State $L \gets \left\lfloor \nicefrac{N}{3} \right\rfloor$;
            \State $\symbf{H}_{\symbf{y}} \gets
                \begin{bmatrix}
                    y_{0} & y_{1} & \cdots & y_{L}\\
                    y_{1} & y_{2} & \cdots & y_{L+1}\\
                    \vdots & \vdots & \ddots & \vdots\\
                    y_{N-L-1} & y_{N-L} & \cdots & y_{N-1}\\
                \end{bmatrix}
            $;
            \State $\symbf{U}, \symbf{\sigma}, \symbf{V} \gets
            \SVD(\symbf{H}_{\symbf{y}})$;
            \If {$M = 0$}
                \State $M \gets \operatorname{MDL}(\symbf{\sigma}, L, N)$;
            \EndIf
            \State $\symbf{U}_{M1}, \symbf{U}_{M2} \gets
                \symbf{U}[:, :M-1], \symbf{U}[:, 1:M]
            $;
            \State $\symbf{z} \gets \textsc{Eigenvalues}\left(\symbf{U}_{M1}^+ \symbf{U}_{M2}^{\vphantom{+}}\right)$;
            \State $\symbf{Z} \gets
                \begin{bmatrix}
                    \symbf{1} & \symbf{z} & {\symbf{z}}^2 & \cdots & {\symbf{z}}^{N}
                \end{bmatrix}\T
            $;
            \State $\bdalpha \gets \symbf{Z}^+ \by$;
            \State $
                \symbf{a}, \symbf{\phi}, \symbf{f}, \symbf{\eta} \gets
                \left\lvert\symbf{\alpha}\right\rvert,
                    \arctan \left(\frac{\Im(\symbf{\alpha})}{\Re(\symbf{\alpha})}\right),
                    \frac{\fsw}{2\pi} \Im \left( \ln \symbf{z} \right) + \foff,
                    -\fsw \Re \left( \ln \symbf{z} \right)
                $;
            \If {$\symbf{\eta}$ contains negative values}
                \State Remove these from $\symbf{\eta}$, and remove the
                corresponding values from
                $\symbf{a}$, $\symbf{\phi}$, and $\symbf{f}$;
            \EndIf
            \State $\bthzero \gets
                \begin{bmatrix}
                    \symbf{a}\T &
                    \symbf{\phi}\T &
                    \symbf{f}\T &
                    \symbf{\eta}\T
                \end{bmatrix}\T
            $;
            \State \textbf{return} $\bthzero$;
        \EndProcedure
        \Statex
        \Procedure{MDL}{$\symbf{\sigma} \in \mathbb{R}^{L+1}, L \in \mathbb{N}, N \in \mathbb{N}$}\label{state:mdl}
            \For {$k = 0, \cdots, L$}
                \State $\operatorname{MDL}_k \gets
                -\ln\left(
                    \frac
                    {\prod_{r=k}^{L-1} \sigma_{r+1}^{\nicefrac{1}{L-k}}}
                    {\frac{1}{L-k} \sum_{r=k}^{L-1} \sigma_{r+1}}
                \right)^{(L-k)N}
                + \frac{1}{2}k(2L - k)\ln N
                $;
                \If {$k > 0$ \textbf{and} $\operatorname{MDL}_k > \operatorname{MDL}_{k-1}$}
                    \State $M \gets k-1$;
                    \State \textbf{break};
                \EndIf
            \EndFor
            \State \textbf{return} $M$;
        \EndProcedure
    \end{algorithmic}
\end{algorithm}


\subsection{The 2D MMEMPM}
\label{subsec:mmempm}
The \ac{MPM} was extended for the consideration of \ac{2D} data by Hua with the
\acfi{MEMPM}~\cite{Hua1992}. The method centers around the enhanced matrix $\EY
\in \mathbb{C}^{\Lone \Ltwo \times (\None - \Lone + 1)(\Ntwo - \Ltwo + 1)}$,
a block-Hankel matrix of the form
\begin{subequations}
    \begin{gather}
        \EY =
        \begin{bmatrix}
            \symbf{H}_{\symbf{y},0} & \symbf{H}_{\symbf{y},1} & \cdots & \symbf{H}_{\symbf{y},\None - \Lone} \\
            \symbf{H}_{\symbf{y},1} & \symbf{H}_{\symbf{y},2} & \cdots & \symbf{H}_{\symbf{y},\None - \Lone + 1} \\
            \vdots & \vdots & \ddots & \vdots \\
            \symbf{H}_{\symbf{y},\Lone - 1} & \symbf{H}_{\symbf{y},\Lone} & \cdots & \symbf{H}_{\symbf{y},\None - 1}
        \end{bmatrix}, \\
        \def\arraystretch{1.3}
        \symbf{H}_{\symbf{y},\none} =
        \begin{bmatrix}
            y_{ \none, 0 } & y_{ \none, 1 } & \cdots & y_{ \none, \Ntwo - \Ltwo } \\
            y_{ \none, 1 } & y_{ \none, 2 } & \cdots & y_{ \none, \Ntwo - \Ltwo + 1 } \\
            \vdots & \vdots & \ddots & \vdots \\
            y_{ \none, \Ltwo - 1 } & y_{ \none, \Ltwo } & \cdots & y_{ \none, \Ntwo - 1 }
        \end{bmatrix}.
    \end{gather}
\end{subequations}
In an equivalent fashion to \cref{eq:HX-decomp},
$\symbf{H}_{\symbf{x},\none}$, the noiseless equivalent to
$\symbf{H}_{\symbf{y},\none}$, can be expressed as
\begin{equation}
    \symbf{H}_{\symbf{x},\none} =
        \symbf{Z}^{(2)}_{\text{L}}
        \symbf{A}
        {\symbf{Z}^{(1)}_{\text{D}}}^{\none}
        \symbf{Z}^{(2)}_{\text{R}}.
\end{equation}
This then enables the noiseless enhanced matrix to be decomposed as follows:
\begin{subequations}
    \begin{gather}
        \symbf{E}_{\symbf{X}} =
        \symbf{E}_{\text{L}}
        \symbf{A}
        \symbf{E}_{\text{R}},\\
        \mathbb{C}^{\Lone \Ltwo \times M} \ni
        \symbf{E}_{\text{L}} =
        \begin{bmatrix}
            \symbf{Z}^{(2)}_{\text{L}} \\
            \symbf{Z}^{(2)}_{\text{L}} \symbf{Z}^{(1)}_{\text{D}} \\
            \vdots \\
            \symbf{Z}^{(2)}_{\text{L}} {\symbf{Z}^{(1)}_{\text{D}}}^{\Lone - 1} \\
        \end{bmatrix},\label{eq:EL}\\
        \mathbb{C}^{M \times \left(\None - \Lone + 1\right)\left(\Ntwo - \Ltwo + 1\right)} \ni
        \symbf{E}_{\text{R}} =
        \begin{bmatrix}
            \symbf{Z}^{(2)}_{\text{R}} &
            \symbf{Z}^{(1)}_{\text{D}} \symbf{Z}^{(2)}_{\text{R}} &
            \cdots &
            {\symbf{Z}^{(1)}_{\text{D}}}^{\None - \Lone} \symbf{Z}^{(2)}_{\text{R}} \\
        \end{bmatrix}.
    \end{gather}
\end{subequations}
As was the case in the \ac{MPM}, \ac{SVD} can be utilised to generate a
filtered matrix $\EYtilde$ with its rank reduced to $M$, in accordance with the
\ac{EYM} theorem:
\begin{equation}
    \EYtilde =
        \symbf{U}_M^{\vphantom{\dagger}}
        \symbf{\Sigma}_M^{\vphantom{\dagger}}
        \symbf{V}_M^{\dagger}
\end{equation}
Due to the large size of the enhanced matrix, a vast improvement
in the speed of the \ac{MEMPM} can be realised when, rather than compute the
\ac{SVD} in its entirety, a \emph{truncated} \ac{SVD} is computed, in
which only the first $M$ components of the \ac{SVD} are
determined using iterative approaches like the Rayleigh-Ritz/Arnoldi
method~\cite{Arnoldi1951,svds}.

If the conditions $\Nd - L^{(d)} + 1 \geq M\ \forall d \in \lbrace 1, 2
\rbrace$ are met, $\range\left(\symbf{U}_M\right) =
\range\left(\symbf{E}_{\text{L}}\right)$. This implies that there is some
nonsingular matrix $\symbf{T} \in \mathbb{C}^{M \times M}$ such that
\begin{equation}
    \symbf{U}_M = \symbf{E}_{\text{L}} \symbf{T}.
\end{equation}
Now consider the following two matrices:
\begin{subequations}
    \begin{gather}
        \symbf{U}_{M1} = \symbf{E}^{\vphantom{(1)}}_{\text{L}1} \symbf{T},\\
        \symbf{U}_{M2} = \symbf{E}^{\vphantom{(1)}}_{\text{L}1} \symbf{Z}^{(1)}_{\text{D}} \symbf{T},\\
        \mathbb{C}^{\Lone \left(\Ltwo - 1\right) \times M} \ni
        \symbf{E}_{\text{L}1} =
        \begin{bmatrix}
            \symbf{Z}^{(2)}_{\text{L}} \\
            \symbf{Z}^{(2)}_{\text{L}} \symbf{Z}^{(1)}_{\text{D}} \\
            \vdots \\
            \symbf{Z}^{(2)}_{\text{L}} {\symbf{Z}^{(1)}_{\text{D}}}^{\Lone - 2}
        \end{bmatrix}.
    \end{gather}
\end{subequations}
$\symbf{E}_{\text{L}1}$ is derived from $\symbf{E}_{\text{L}}$ through the
removal of its last $\Ltwo$ rows.
As such, $\symbf{U}_{M1}$ and $\symbf{U}_{M2}$ correspond the $\symbf{U}_M$
with the last and first $\Ltwo$ rows removed, respectively. The matrix pencil
for $\symbf{U}_{M1}$ and $\symbf{U}_{M2}$ can be expressed as
\begin{equation}
    \symbf{U}_{M1} - \lambda \symbf{U}_{M2} =
    \symbf{E}_{\text{L}1} \left( \symbf{Z}^{(1)}_{\text{D}} - \lambda \symbf{I}_M \right) \symbf{T}.
\end{equation}
As seen previously, this matrix structure implies that the signal poles in the
first dimension $\bdzone$ are the solutions of the generalised eigenvalue
problem for $\symbf{U}_{M1} - \lambda \symbf{U}_{M2}$, such that they are
the eigenvalues of $\symbf{U}_{M1}^{+} \symbf{U}_{M2}^{\vphantom{+}}$.

To extract the signal poles in the other dimension, $\bdztwo$, the permutation
matrix $\symbf{P} \in \mathbb{R}^{\Lone \Ltwo \times \Lone \Ltwo}$ is defined:
\begin{equation}
    \symbf{P} =
    {\footnotesize
    \begin{bmatrix}
        \symbf{e}\left(1\right) \\
        \symbf{e}\left(1 + \Ltwo\right) \\
        \vdots \\
        \symbf{e}\left(1 + \left(\Lone - 1\right)\Ltwo\right) \\
        \symbf{e}\left(2\right) \\
        \symbf{e}\left(2 + \Ltwo\right) \\
        \vdots \\
        \symbf{e}\left(2 + \left(\Lone - 1\right)\Ltwo\right) \\
        \vdots \\
        \vdots \\
        \symbf{e}\left(\Ltwo\right) \\
        \symbf{e}\left(2\Ltwo\right) \\
        \vdots \\
        \symbf{e}\left(\Lone \Ltwo\right) \\
    \end{bmatrix}.
    }
\end{equation}
$\symbf{e}\left(i\right) \in \mathbb{R}^{\Lone \Ltwo}$ corresponds to a unit row
vector comprising zeros except for $e_i = 1$.
Multiplying $\symbf{E}_{\text{L}}$ by the permutation matrix leads to a matrix
in which the roles of the two sets of signal poles are effectively swapped:
\begin{equation}
    \symbf{E}_{\text{LP}} \coloneq \symbf{P} \symbf{E}_{\text{L}} =
    \begin{bmatrix}
        \symbf{Z}^{(1)}_{\text{L}} \\
        \symbf{Z}^{(1)}_{\text{L}} \symbf{Z}^{(2)}_{\text{D}} \\
        \vdots \\
        \symbf{Z}^{(1)}_{\text{L}} {\symbf{Z}^{(2)}_{\text{D}}}^{\Ltwo - 1} \\
    \end{bmatrix}.\label{eq:ELP}
\end{equation}
Note the similarity of \cref{eq:ELP} with \cref{eq:EL}, which
implies that with the same reasoning as above, $\bdztwo$ can be derived
by extracting the eigenvalues of $\symbf{U}_{M\text{P}1}^+
\symbf{U}_{M\text{P}2}^{\vphantom{+}}$, where $\symbf{U}_{M\text{P}1}$ and
$\symbf{U}_{M\text{P}2}$ correspond to $\symbf{P} \symbf{U}_M$
with the last and first $\Lone$ rows removed, respectively.

\begin{figure}
    \centering
    \includegraphics{mmempm_poles/mmempm_poles.pdf}
    \caption[
        A comparison of the structure of the matrix $\symbf{G}$ from the
        \acs{MMEMPM}, for the cases of an \acs{FID} with distinct and repeated
        signal poles in $\Fone$.
    ]{
        A comparison of the structure of the matrix $\symbf{G}$ from the
        \acs{MMEMPM} (\cref{eq:G}), for the cases of an \acs{FID} with \textbf{a.} distinct
        signal poles in $\Fone$ and \textbf{b.} repeated signal poles.
        \textbf{a1.} Magntiude-mode spectrum of a hypercomplex \ac{2D}
        \ac{FID}, with $M=5$, and all indirect-dimension frequencies having
        distinct values.
        \textbf{b1.} An equivalent spectrum with two pairs of the $5$
        indirect-dimension frequencies being identical ($\lbrace 1, 4 \rbrace$
        and  $\lbrace 2, 5 \rbrace$).
        \textbf{a2.} A representation of the matrix
        $\lvert \symbf{G} \rvert$ for the dataset depicted in a1. All values
        which are greater than \num{1e-10} are coloured black, while those that
        are less than \num{1e-10} are white, though note that all the black
        values are $\gg \num{1e-10}$.
        \textbf{b2.} An analogous representation for the dataset in b1. Note
        that $\symbf{G}$ is no longer diagonal, but possesses non-zero
        off-diagonal elements in agreement with signals with equivalent poles.
        With an appropriate re-ordering of the rows and columns (i.e. swapping
        the rows and columns of 2 and 4), this could be recast as a
        block-diagonal matrix of the form in \cref{eq:block-G}, featuring 3
        blocks, one which is $1 \times 1$, and two which are  $2 \times 2$.
    }
    \label{fig:mmempm-poles}
\end{figure}
In the original account on the \ac{MEMPM}, the final stage employed a
pairing algorithm in order to assign the uncorrelated signal poles in $\bdzone$
with those in $\bdztwo$~\cite{Hua1992}. The \emph{modified} \ac{MEMPM} (\acs{MMEMPM}) was
developed in order to overcome two issues with the pairing algorithm: (a) it is
computationally expensive, and (b) it is prone to return incorrect
pairings~\cite{Chen2007}.
The procedure for extracting the paired poles differs slightly depending on
whether all the signal poles in $\symbf{z}^{(1)}$ are distinct or not.
As well as the eigenvalues of
$\symbf{U}^{+}_{M1}\symbf{U}_{M2}^{\vphantom{+}}$, the \ac{MMEMPM} requires
the corresponding eigenvectors too, contained in the matrix
$\symbf{W}^{(1)}$. Assuming that there are no repeated poles in
$\bdzone$\footnote{
    For the purposes of \ac{FID} estimation, it is deemed that a given pair
    $i,j \in \lbrace 1, \cdots, M \rbrace$ of poles is repeated if their
    frequencies satisfy $\lvert \fone_i - \fone_j \rvert <
    \nicefrac{\fswone}{\None}$ (recall \cref{eq:poles-to-f}, which states the
    relationship between a signal pole and its frequency).
    \label{fn:similar-freqs}
}, the following holds:
\begin{equation}
    \bdZtwo_{\text{D}} \equiv \symbf{G} \coloneq
        {\symbf{W}^{(1)}}^{-1}
        \symbf{U}_{M\text{P}1}^+
        \symbf{U}_{M\text{P}2}^{\vphantom{+}}
        \symbf{W}^{(1)},
        \label{eq:G}%
\end{equation}
such that the second-dimension signal poles can be extracted from the main
diagonal of $\symbf{G}$ (see Figures \ref{fig:mmempm-poles}.a1 and
\ref{fig:mmempm-poles}.a2).
If some values in $\bdzone$ are repeated, the matrix $\symbf{G}$ is
no longer diagonal, but instead and can be expressed as block-diagonal;
assuming there are $R < M$ unique signal poles $\symbf{G}$ may be expressed in
the following form, allowing for a shuffling of rows and columns:
\begin{equation}
    \symbf{G} =
    \begin{bmatrix}
        \symbf{G}_1 & \symbf{0} & \cdots & \symbf{0} \\
        \symbf{0} & \symbf{G}_2 & \cdots & \symbf{0} \\
        \vdots & \vdots & \ddots & \vdots \\
        \symbf{0} & \symbf{0} & \cdots & \symbf{G}_R
    \end{bmatrix},
    \label{eq:block-G}%
\end{equation}%
where $\symbf{G}_r \in \mathbb{C}^{h_r \times h_r}$ is a sub-matrix corresponding
to the $r$\textsuperscript{th} unique signal pole with multiplicity $h_r$ (see
Figures \ref{fig:mmempm-poles}.b1 and \ref{fig:mmempm-poles}.b2). Note that
$\sum_{r=1}^R h_r = M$.  When $h_r > 1$, the subset of poles $\bdztwo_r$ are
determined by computing the eigenvalues of the matrix $\symbf{G}_r$

With the signal poles in both dimensions determined, the complex amplitudes are
finally computed as follows:
\begin{equation}
    \symbf{\alpha} = \diag \left( \symbf{E}_{\text{L}}^+ \symbf{E}_{\symbf{Y}}^{\vphantom{+}} \symbf{E}_{\text{R}}^+ \right).
    \label{eq:complex-amplitudes-2d}%
\end{equation}

See \cref{alg:mmempm} for a pseudo-code outline, and \cref{lst:mmempm} for a
\Python implementation of the \ac{MMEMPM}.

\subsection{Model Order Selection}
\label{subsec:model-order}
The \ac{MPM} and \ac{MMEMPM} operate under the assumption that the model order
$M$ is known/has been predicted.
It is possible that an individual inspecting the \ac{NMR} spectrum could
predict $M$ based on the number of peaks visible, however subjective means of
predicting model order are typically viewed as disadvantageous as they have bias
associated with them. On top of this, scenarios may crop up where a user is
unable to recognise that certain heavily overlapping peaks in the spectrum
constitute a more than one signal, leading to under-estimates of $M$.
There are various non-subjective criteria which have been established for
estimating the model order of a given signal, with probably the two most
prominent being the \ac{AIC}~\cite{Akaike1974} and
\ac{MDL}~\cite{Schwarz1978,Rissanen1978}. Both of these consider a family of
potential models which describe a given set of observations, parametrised by
the vector $\bth$. For the purpose of \ac{1D} \ac{FID} estimation, the family
of potential models comprise \cref{eq:x} with variable $M$. Both the \ac{AIC}
and \ac{MDL} take the same general form:
\begin{equation}
    \mathcal{C}(k) = -c \ln \left(\mathcal{L} \left(\bthstar | \by \right)
    \right) + \mathcal{P}(k) \text{ with } \bthstar \in \mathbb{R}^{4k}
    \quad \forall k \in \lbrace 0, 1, \cdots \rbrace.
\end{equation}
 $\mathcal{L} \left(\bthstar |
\by \right)$ is the likelihood
function of the model with order $k$ at the \ac{MLE}, $c \in
\mathbb{R}_{>0}$ is a scaling constant, and $\mathcal{P}$ is a penalising
function, which acts to correct
for bias. As the model order increases, the likelihood function at the \ac{MLE}
will increase in size, as a model with more parameters will be able to fit a
given dataset more accurately. However, as the model order increases, there will
become a point where practically all of the deterministic part of the signal
has been incorporated into the model, and increasing the model order further
leads to a model which also features noise components. The penalising
term $\mathcal{P}(k)$, which always increases with $k$, is introduced in order to
obtain a parsimonious model order estimate. Wax and Kailath derived an
expression for the likelihood at the \ac{MLE} for models comprising a summation
of complex sinusoids~\cite{Wax1985}\footnote{
    The expression in Wax and Kailath's paper considers the eigenvalues of the
    covariance matrix for the signal, rather than the singular values of $\Hy$;
    these are equivalent.
}:
\begin{equation}
    \mathcal{L}\left(\bthstar \in \mathbb{R}^{4k} | \by\right) = \left(
        \frac{
            \prod_{r=k}^{L-1} \sigma_{r+1}^{\nicefrac{1}{L-k}}
        }{
            \frac{1}{L-k} \sum_{r=k}^{L-1} \sigma_{r+1}
        }
        \right)^{(L - k) N}
        \quad \forall k \in \lbrace 0, 1, \cdots, L - 1 \rbrace.
        \label{eq:wax-pdf}
\end{equation}
$\symbf{\sigma} \in
\mathbb{R}^L$ is the set of singular values of $\symbf{H}_{\symbf{y}}$,
in decreasing order. The explicit forms of the \ac{AIC} and \ac{MDL} are
\begin{subequations}
    \begin{gather}
        \operatorname{AIC}(k) = -2 \ln\left( \mathcal{L} \left(\bthstar | \by\right) \right) + 2k(2 L - k), \\
        \operatorname{MDL}(k) = -\ln\left( \mathcal{L} \left(\bthstar | \by\right) \right) + \tfrac{1}{2} k(2 L - k) \ln N. \label{eq:mdl}
    \end{gather}
\end{subequations}
The \ac{AIC} has been shown to be inconsistent since it tends to overestimate
the model order as the number of samples increases~\cite{Wax1985}. For this
reason, the \ac{MDL} has found greater favour in signal processing
applications, and is employed in this work\footnote{
    In practice, rather than compute the global minimum of the \ac{MDL}, the
    first relative minimum is determined (see \cref{alg:mpm},
    \cref{state:mdl}). When $k$ is very large, it has been noticed that the
    \ac{MDL} can spuriously jump in value. This often occurs when \acp{FID}
    with very high \acp{SNR} are analysed; the singular values involved
    can tend so closely to zero that issues related to the instability of
    floating-point arithmetic manifest.
    \label{fn:argrelmin}
}:
\begin{equation}%
    M = \argmin_{k \in \mathbb{N}_0 :\ k < L} \operatorname{MDL} (k).%
\end{equation}%
\begin{figure}%
    \centering
    \includegraphics{mdl/mdl.pdf}
    \caption[%
        A visualisation of the behaviour of the \acs{MDL} for three different
        \acsp{FID} comprising the same deterministic component, but with
        different noise variances.
    ]{%
        A visualisation of the behaviour of the \acs{MDL} for three different
        \acsp{FID} comprising the same deterministic component ($\bx$) but
        with noise instances of differing variances. The model used
        to construct the \acp{FID} features 7
        signals. The three \acsp{SNR} used were
        \qty{7}{\deci\bel} (red), \qty{12}{\deci\bel} (blue), and
        \qty{20}{\deci\bel} (yellow). The \acsp{FID} were generated with $N
        = 256$.
        \textbf{a.} Spectra of the three \acsp{FID}.
        \textbf{b.} The values of the 14 most significant singular values
        associated with the Hankel matrix $\Hy$, with the
        pencil parameter $L$ set to $\lfloor \nicefrac{N}{3} \rfloor =
        85$.
        \textbf{c.} Square points with dotted lines: The negative log-likeliood
        at the \ac{MLE}, i.e. the first term of \cref{eq:mdl}.
        Grey line: the penalty component of the \ac{MDL}, i.e. the second
        term in \cref{eq:mdl}.
        Circular points with solid lines: the \ac{MDL}.
        Stars denote the minimum of the \ac{MDL} for a given \ac{FID}. The
        \qty{20}{\deci\bel}
        signal is correctly deemed to have a model order of 7, while the other
        two are underestimated (predicted model orders are 5 and 3 for the
        \qty{12}{\deci\bel} and \qty{7}{\deci\bel} \acsp{FID}, respectively).
    }%
    \label{fig:mdl}%
\end{figure}%
Applying the \ac{MDL} for model order selection, and subsequently using the
\ac{MPM} for parameter estimation is the basis of the \ac{ITMPM} from Pines and
co-workers~\cite{Lin1997}.
\Cref{fig:mdl} illustrates the form of the \ac{MDL} for three \acp{FID}
with equivalent underlying models, comprising $7$ signals, but with noise
instances of differing variances. The first 14 singular values of $\Hy$
are plotted in \cref{fig:mdl}.b, where it can be seen that beyond the first 7,
which account for signal components, the subsequent singular values decrease
at a far slower rate. The noise subspace for \acp{FID} with higher \acp{SNR}
have singular values which are smaller in magnitude and more
consistent, such that distinguishing the noise and signal subspaces is an
easier task (cf. the yellow and red lines). As such, it is perhaps
unsurprising that the \ac{MDL} is more likely to provide a faithful estimate of
the \ac{FID}'s model order when the \ac{SNR} is higher (see \cref{fig:mdl}.c).
