\chapter{Additional Theory}
\label{chap:additional-theory}

\section{Mathematical definitions}

Descriptions of linear algebra and statistics concepts that are referred to in
this thesis are provided here. More detail can be found in numerous relevant
texts\cite{Strang2018,Pawitan2001}.

\subsection{Linear algebra}
\label{subsec:linear-algebra}

\subsubsection{Matrix Rank}
The rank of a matrix is defined as the dimension of the vector space spanned by
its rows; this is identical to the dimension of the vector space spanned by
its columns. Equivalently, rank can be thought of as the number of linearly
independent row/column vectors present in the matrix. For a matrix $\symbf{X}
\in \mathbb{F}^{m \times n}$, the largest possible rank\,---\,often referred to
as ``full rank''\,---\,is $\min(m, n)$.

\subsubsection{\Acf{SVD}}
\ac{SVD} is a generalisation of eigendecomposition\,---\,applicable only to
square, diagonalisable matrices\,---\,for any general matrix $\symbf{X} \in
\mathbb{C}^{m \times n}$. The \ac{SVD} is a factorisation given by
\begin{equation}
    \symbf{X} = \symbf{U} \symbf{\Sigma} \symbf{V}^{\dagger}.
\end{equation}
The matrices that make up the factorisation are as follows
\begin{itemize}
    \item $\symbf{\Sigma} \in \mathbb{C}^{m \times n}$ is a rectangular
        diagonal matrix with diagonal elements comprising the \emph{singular
        values} of $\symbf{X}$ in descending order of magnitude. The singular
        values are the square roots of the non-zero eigenvalues of both
        $\symbf{X}^{\dagger}\symbf{X}$ and $\symbf{X}\symbf{X}^{\dagger}$.
    \item $\symbf{U} \in \mathbb{C}^{m \times m}$ is a unitary matrix whose columns
        comprise the \emph{left singular vectors} of $\symbf{X}$. The left
        singular vectors are the eigenvectors of the matrix
        $\symbf{X}\symbf{X}^{\dagger}$.
    \item $\symbf{V} \in \mathbb{C}^{n \times n}$ is a unitary matrix whose columns
        comprise the \emph{right singular vectors} or $\symbf{X}$. These
        are the eigenvectors of the matrix $\symbf{X}^{\dagger}\symbf{X}$.
\end{itemize}
One fundamental property of the \ac{SVD} is that the number of non-zero
singular values is equivalent to the rank of the matrix. As the \ac{EYM}
theorem highlights, the \ac{SVD} is valuable in constructing low-rank
approximations of matrices, with applications in various fields such as signal
processing, (see \cref{subsec:mpm}) and data compression\cite{Jaradat2021}.

\subsubsection{Special matrices}
A \emph{Hankel matrix} is a matrix in which each \emph{ascending} diagonal from
left to right possesses identical elements. While Hankel matrices are often
defined to be square, in this work such a restriction is not applied. Given a
matrix $\symbf{X} \in \mathbb{F}^{M \times N}$, the matrix is Hankel if
\begin{equation}
    x_{m,n} = x_{m+1,n-1}\quad
    \forall m \in \lbrace 1, \cdots, M-1 \rbrace,
    \ \forall n \in \lbrace 2, \cdots, N \rbrace.
\end{equation}
Similarly, a \emph{Toeplitz matrix} is a matrix in which every \emph{descending}
diagonal from left to right possesses identical elements, i.e.
\begin{equation}
    x_{m,n} = x_{m+1,n+1}\quad
    \forall m \in \lbrace 1, \cdots, M-1 \rbrace,
    \ \forall n \in \lbrace 1, \cdots N-1 \rbrace.
\end{equation}
A \emph{Vandermonde matrix} is a matrix with rows that feature a geometric
progression, i.e.
\begin{equation}
    x^{\vphantom{n}}_{m,n\vphantom{1}} = x_{m,1}^n\quad
    \forall m \in \lbrace 1, \cdots, M \rbrace,
    \ \forall n \in \lbrace 1, \cdots N \rbrace.
\end{equation}

\subsection{Statistics and Probability}

\subsubsection{Probability Density Function (\acs{PDF})}
The \ac{PDF} $p(x) : \mathbb{R} \rightarrow \mathbb{R}$ is a function over a
continuous sample space which provides relative likelihoods between potential
values of $x$.  The probability $P \in [0, 1]$ that a random sample obeying a
known distribution lies withing the range $[x_a, x_b]$ is given by the integral
\begin{equation}
    P(x_a \leq x \leq x_b) = \int_{x_a}^{x_b} p(x) \mathrm{d}x.
\end{equation}
The integral of $p(x)$ over the entire sample space is defined to be
unity. The probability of a specific value $P(x)$ is always $0$ by definition,
since the width of the region of integration is $0$. The \ac{PDF} can be
extended for the consideration of multiple variables; in this case
$p(\symbf{x}): \mathbb{R}^n \rightarrow \mathbb{R}$ is commonly referred to as
a \emph{multivariate} \ac{PDF}.

\subsubsection{Likelihood Function and Maximum Likelihood Estimate (\acs{MLE})}
The likelihood function $\mathcal{L}(\bdthY)$ is the probability density of
some observed data, when viewed from the perspective of the parameters of a
model (i.e. the data is taken as a fixed parameter, and the model parameters
are taken as variables). The \ac{MLE} is the set of parameters which maximises
the likelihood function:
\begin{equation}
    \bthstar = \argmax_{\bth} \mathcal{L}(\bdthY).
\end{equation}

\section{Further Information about \acs{NLP}}
\subsection{Model Derivatives}
\label{sec:derivatives}
The complete set of first and second derivatives of a particular element of the
model $x \coloneq \xnonenD$ (\cref{eq:x}) is as follows
$\forall m \in \lbrace 1, \cdots, M \rbrace$,
$\forall d, d^{\prime} \in \lbrace 1, \cdots, D \rbrace$:
\paragraph{First derivatives}
\begin{subequations}
    \begin{gather}
        \xderiv{\theta_m} \equiv
            \xderiv{a_m} =
            \frac{x}{a_m},
            \label{eq:deriv-a}\\
        \xderiv{\theta_{m + M}} \equiv
            \xderiv{\phi_m} =
            \iu x,\\
        \xderiv{\theta_{m + (d + 1)M}} \equiv
            \xderiv{\fdm} =
            2 \pi \iu \Dtd \nd x,\\
        \xderiv{\theta_{m + (d + D + 1)M}} \equiv
            \xderiv{\etadm} =
            - \Dtd \nd x.
    \end{gather}
    \label{eq:first-derivs}
\end{subequations}
\paragraph{Second derivatives}
\begin{subequations}
    \begin{gather}
        \xderivtwosame{\theta_{m}} \equiv
            \xderivtwosame{a_m} =
            0,
            \label{eq:amp-second-deriv}\\
        \xderivtwodiff{\theta_{m}}{\theta_{m + M}} \equiv
            \xderivtwodiff{a_m}{\phi_m} =
            \frac{\iu x}{a_m},\\
        \xderivtwodiff{\theta_{m}}{\theta_{m + (d + 1)M}} \equiv
            \xderivtwodiff{a_m^{\vphantom{(d)}}}{\fdm} =
            \frac{2 \pi \iu \Dtd \nd x}{a_m},\\
        \xderivtwodiff{\theta_{m}}{\theta_{m + (d + D + 1)M}} \equiv
            \xderivtwodiff{a_m^{\vphantom{(d)}}}{\etadm} =
            \frac{-\Dtd \nd x}{a_m},\\
        \xderivtwosame{\theta_{m + M}} \equiv
            \xderivtwosame{\phi_m} =
            -x,\\
        \xderivtwodiff{\theta_{m + M}}{\theta_{m + (d + 1)M}} \equiv
            \xderivtwodiff{\phi_m^{\vphantom{(d)}}}{\fdm} =
            -2 \pi \Dtd \nd x,\\
        \xderivtwodiff{\theta_{m + M}}{\theta_{m + (d + D + 1)M}} \equiv
            \xderivtwodiff{\phi_m^{\vphantom{(d)}}}{\etadm} =
            -\iu \Dtd \nd x,\\
        \xderivtwodiff{\theta_{m + (d + 1)M}}{\theta_{m + (d^{\prime} + 1)M}} \equiv
            \xderivtwodiff{\fdm}{\fdmp} =
            -4\pi^2 \left(\Dtd \nd \right) \left(\Dtdp \ndp \right) x,\\
        \xderivtwodiff{\theta_{m + (d + 1)M}}{\theta_{m + (d^{\prime} + D + 1)M}} \equiv
            \xderivtwodiff{\fdm}{\etadmp} =
            -2 \pi \iu \left(\Dtd \nd \right) \left(\Dtdp \ndp \right) x,\\
        \xderivtwodiff{\theta_{m + (d + D + 1)M}}{\theta_{m + (d^{\prime} + D + 1)M}} \equiv
            \xderivtwodiff{\etadm}{\etadmp} =
            \left(\Dtd \nd \right) \left(\Dtdp \ndp \right) x,\\
        \xderivtwodiff{\theta_{i}}{\theta_{j}} =
            \xderivtwodiff{\theta_{j}}{\theta_{i}},
            \label{eq:symmetric-second-derivs}\\
        \xderivtwodiff{\theta_{i}}{\theta_{j}} = 0\ \text{ if not specified above.}
        \label{eq:zero-second-deriv}
    \end{gather}
    \label{eq:second-derivs}
\end{subequations}

\subsection{Estimation Errors}
\label{subsec:errors}
A measure of the degree of uncertainty in the parameter estimates can be obtained
by computing the \emph{standard errors} associated with the \ac{NLP} routine.
Standard errors are related  to the \emph{observed Fisher information matrix}
at convergence\cite[Section 2.7]{Pawitan2001}:
\begin{equation}
    \symbf{\epsilon}\left(\bthstar\right) = \sqrt{\diag\left(\symbf{I}\left( \bthstar \right)^{-1}\right)},
\end{equation}
where the observed Fisher Information matrix contains the negative partial second
derivatives of the log-likelihood with respect to $\bth$:
\begin{equation}
    \symbf{I}\left(\bth\right)_{i, j} =
        -\frac
        {\partial^2 \ell \left( \bdthY \right)}
        {\partial \theta_i \partial \theta_j}.
\end{equation}
Recalling the form of the log-likelihood given by \cref{eq:log-likeihood},
the elements of $\symbf{I}\left(\bth\right)$ are
\begin{equation}
    \symbf{I}\left(\bth\right)_{i, j} =
        -\frac{1}{\sigma^2}
        \Re
        \biggl(
            \left\langle
                \frac{\partial \bX}{\partial \theta_i},
                \frac{\partial \bX}{\partial \theta_j}
            \right\rangle
            -
            \left\langle
                \left(\bY - \bX\right),
                \frac{\partial^2 \bX}{\partial \theta_i \partial \theta_j}
            \right\rangle
        \biggl),
\end{equation}
which very closely resembles the Hessian of $\bth$:
\begin{equation}
    \symbf{I}\left(\bth\right)_{i,j} =
        \frac{1}{2 \sigma^2}
            \bdHth_{i,j}.
\end{equation}
The standard errors therefore take the form
\begin{equation}
    \symbf{\epsilon}\left(\bthstar\right) =
        \sqrt{
            2\sigma^2 \diag \left(
                \bdHthstar^{-1}
            \right)
        }.
\end{equation}
The mean and variance of the noise are $0$ and $2\sigma^2$, respectively,
leading to:
\begin{equation}
    2 \sigma^2 = \frac{1}{\mathfrak{N} - 1}
    \left\lVert \bW \right\rVert^2 =
    \frac{1}{\mathfrak{N} - 1} \left \lVert
        \bY - \bXthstar
    \right \rVert^2.
\end{equation}
Finally a useable expression for the standard errors is arrived at:
\begin{equation}
    \symbf{\epsilon}\left(\bthstar\right) =
        \sqrt{
            \frac
            {
                \Fthstar \diag \left(
                    \bdHthstar^{-1}
                \right)
            }
            {\mathfrak{N} - 1}
        }
\end{equation}



\section{Multidimensional \aclp{VE}}
\label{sec:multidim-ve}
The \ac{VE} concept (\cref{subsec:ve}) can be generalised to any number
of dimensions, assuming that a pair of amplitude-modulated signals exist for
each indirect-dimension. Thus a set of $2^{D-1}$ signals is required for a
$D$-dimensional \ac{FID}.
For the \ac{2D} case, this corresponds to the pair of signals $\lbrace
\bY^{\cos}, \bY^{\sin} \rbrace$, given by \cref{eq:general-fid} with $D=2$ and
$\zeta = \lbrace \cos(\cdot), \sin(\cdot) \rbrace$, taking the forms (with
noise neglected)
\begin{subequations}
    \begin{gather}
        y^{\cos}_{\none,\ntwo} =
            \xi^{\vphantom{(1)}}_{\none,\ntwo}
            c^{(1)}_{\none,\ntwo}
            \left(c^{(2)}_{\none,\ntwo} + \iu s^{(2)}_{\none,\ntwo}\right),\\
        y^{\sin}_{\none,\ntwo} =
            \xi^{\vphantom{(1)}}_{\none,\ntwo}
            s^{(1)}_{\none,\ntwo}
            \left(c^{(2)}_{\none,\ntwo} + \iu s^{(2)}_{\none,\ntwo}\right),\\
        \xi^{\vphantom{(1)}}_{\none,\ntwo} =
            \sum_m a_m \exp\left(-\etaonem \none \Dtone -\etatwom \ntwo \Dttwo\right),\\
        (c/s)^{(1/2)}_{\none,\ntwo} =
            \sum_m \cos / \sin \left(2 \pi f^{(1/2)} n^{(1/2)} \Updelta^{(1/2)}\right),\\
        \forall M \in \lbrace 1, \cdots M \rbrace,
        \ \forall \none \in \lbrace 0, \cdots \None - 1 \rbrace,
        \ \forall \ntwo \in \lbrace 0, \cdots \Ntwo - 1 \rbrace,\notag
    \end{gather}
\end{subequations}
where, as with the \ac{1D} case in the main text, it has been assumed the data
is phased, such that $\phi_m = 0\ \forall m$.
Four matrices $\symbf{\psi}_{\pm\pm}$ are then constructed of the form
\begin{equation}
    \begin{gathered}
        \psi_{\pm\pm, \none, \ntwo} =
            \xi_{\none,\ntwo}^{\vphantom{(1)}}
            \left(c^{(1)}_{\none,\ntwo} \pm^{(1)} \iu s^{(1)}_{\none,\ntwo}\right)
            \left(c^{(2)}_{\none,\ntwo} \pm^{(2)} \iu s^{(2)}_{\none,\ntwo}\right)\\
         \equiv
             \Re\left( y^{\cos}_{\none,\ntwo} \right)
             \pm^{(1)} \pm^{(2)} -
             \Im\left( y^{\sin}_{\none,\ntwo} \right)
             + \iu \left(
             \pm^{(1)}
             \Re\left( y^{\sin}_{\none,\ntwo} \right)
             \pm^{(2)}
             \Im\left( y^{\cos}_{\none,\ntwo} \right)
             \right),
    \end{gathered}
\end{equation}
from which the matrices $\symbf{T}_{1 \rightarrow 4} \in \mathbb{C}^{2 \None
\times 2 \Ntwo}$ are generated:
\begin{subequations}
    \begin{gather}
        \symbf{T}_1 =
        \begin{bmatrix}
            \symbf{\Psi}_{++} & \symbf{0} \\
            \symbf{0} & \symbf{0}
        \end{bmatrix}, \\
        \symbf{T}_2 =
        \begin{bmatrix}
            \symbf{0} & \symbf{0} \\
            \symbf{\Psi}_{-+}^{\leftrightsquigarrow (1)} & \symbf{0}
        \end{bmatrix}^{\circlearrowright (1)}, \\
        \symbf{T}_3 =
        \begin{bmatrix}
            \symbf{0} & \symbf{\Psi}_{+-}^{\leftrightsquigarrow (2)} \\
            \symbf{0} & \symbf{0}
        \end{bmatrix}^{\circlearrowright (2)}, \\
        \symbf{T}_4 =
        \begin{bmatrix}
            \symbf{0} & \symbf{0} \\
            \symbf{0} & \symbf{\Psi}_{--}^{\leftrightsquigarrow (1,2)}
        \end{bmatrix}^{\circlearrowright (1,2)}.
    \end{gather}
\end{subequations}
The virtual echo is then given by $\symbf{Y}_{\text{VE}} = \sum_{i=1}^4
\symbf{T}_i$, with the first row and column divided by two. For a full outline
of the 2D filtering procedure, see \cref{alg:filter-2d}.

It is possible to construct a virtual echo using an appropriate set of
phase-modulated signals too, which for the \ac{2D} case would be $\lbrace
\symbf{Y}^{\text{pos}}, \symbf{Y}^{\text{neg}}\rbrace$, given by
\cref{eq:general-fid} with $D=2$ and  $\zeta = \lbrace \exp(\iu \cdot),
\exp(-\iu\cdot)\rbrace$. These can be used to generate an amplitude modulated pair via
\begin{subequations}
    \begin{gather}
        \symbf{Y}^{\text{cos}} = \frac{\symbf{Y}^{\text{pos}} + \symbf{Y}^{\text{neg}}}{2},\\
        \symbf{Y}^{\text{sin}} = \frac{\symbf{Y}^{\text{pos}} - \symbf{Y}^{\text{neg}}}{2\iu},
    \end{gather}
\end{subequations}
which can then be processed as described above to form $\bY_{\text{VE}}$.

\clearpage
\section{Additional algorithms}
\label{sec:algs}
\begin{algorithm}[h!]
    \caption[
        The \acs{MMEMPM}.
    ]{
        The \acs{MMEMPM}. \textsc{TruncatedSVD} is a routine which computes the
        first $M$ \ac{SVD} components of a matrix, using some iterative
        approach such as the Rayleigh-Ritz method.
    }
    \label{alg:mmempm}
    \begin{algorithmic}[1]
        \Procedure {MMEMPM}{$\symbf{Y} \in \mathbb{C}^{\None \times \Ntwo}, M \in \mathbb{N}$}
        \State $\Lone, \Ltwo \gets \left\lfloor \nicefrac{\None}{2} \right\rfloor, \left\lfloor \nicefrac{\Ntwo}{2} \right\rfloor$;
        \For{$\none \gets \lbrace 0, \cdots, \None - 1 \rbrace$}
            \State  $\symbf{H}_{\by,\none} \gets
                \def\arraystretch{1.4}
            \begin{bmatrix}
                y_{\none, 0} &
                y_{\none, 1} &
                \cdots &
                y_{\none, \Ntwo-L^{(2)}}\\
                y_{\none, 1} &
                y_{\none, 2} &
                \cdots &
                y_{\none, \Ntwo-L^{(2)}+1}\\
                \vdots & \vdots & \ddots & \vdots\\
                y_{\none, L^{(2)} - 1} &
                y_{\none, L^{(2)}} &
                \cdots &
                y_{\none, \Ntwo-1}
            \end{bmatrix}
        $;
        \EndFor
        \State $\symbf{E}_{\symbf{Y}} \gets
        \begin{bmatrix}
            \symbf{H}_{\by,0} & \symbf{H}_{\by,1} & \cdots & \symbf{H}_{\by,\None - L^{(1)}}\\
            \symbf{H}_{\by,1} & \symbf{H}_{\by,2} & \cdots & \symbf{H}_{\by,\None - L^{(1)} + 1}\\
            \vdots & \vdots & \ddots & \vdots\\
            \symbf{H}_{\by,L^{(1)} - 1} & \symbf{H}_{\by,L^{(1)}} & \cdots & \symbf{H}_{\by,\None - 1}
        \end{bmatrix}
        $;
        \State $\symbf{U}_M^{\vphantom{\dagger}},
            \symbf{\Sigma}_M^{\vphantom{\dagger}},
            \symbf{V}_M^{\dagger} \gets
            \textsc{TruncatedSVD}\left(\EY, M\right)$;
        \State $\symbf{P} \gets \symbf{0} \in \mathbb{C}^{\Lone \Ltwo \times \Lone \Ltwo}$;
        \State $r \gets 0$
        \For{$i = 0, \cdots, \Ltwo - 1$}
            \For{$j = 0, \cdots, \Lone - 1$}
                \State $c \gets i + j \Ltwo$;
                \State $p_{r, c} \gets 1$;
                \State $r \gets r + 1$;
            \EndFor
        \EndFor
        \State $\symbf{U}_{M1}, \symbf{U}_{M2} \gets \symbf{U}_M\left[ : L^{(1)}(L^{(2)}-1)\right], \symbf{U}_M\left[L^{(2)}:\right]$;
        \Comment{Last/First $L^{(2)}$ rows deleted}
        \State $\symbf{z}^{(1)}, \symbf{W}^{(1)} \gets \textsc{Eigendecomposition}\left( \symbf{U}_{M1}^+ \symbf{U}_{M2}^{\vphantom{+}} \right)$;
        \State $
            \symbf{f}^{(1)},
            \symbf{\eta}^{(1)} \gets
            \left(
                \nicefrac{f_{\text{sw}}^{(1)}}{2 \pi}
            \right)
            \Im \left( \ln \symbf{z}^{(1)} \right) + \foffone,
            -\fswone \Re \left( \ln \symbf{z}^{(1)} \right)
        $;
        \State $\symbf{U}_{M\text{P}} \gets \symbf{P} \symbf{U}_M$;
        \State $
            \symbf{U}_{M\text{P}1},
            \symbf{U}_{M \text{P} 2} \gets
            \symbf{U}_{M \text{P}}\left[ : (L^{(1)}-1)L^{(2)}\right],
            \symbf{U}_{M \text{P}}\left[L^{(1)}:\right]
        $;
        \Comment{Last/First $L^{(1)}$ rows deleted}
        \State $
            \symbf{G} \gets
                {\symbf{W}^{(1)}}^{-1}
                \symbf{U}_{M\text{P}1}^{+}
                \symbf{U}_{M\text{P}2}^{\vphantom{+}}
                \symbf{W}^{(1)}
            $;
        \If{all values in $\bdfone$ are distinct}
        \Comment{See \cref{fn:similar-freqs}, \cpageref{fn:similar-freqs}}
            \State $\bdztwo \gets \diag(\symbf{G})$;
        \Else
            \State $R \gets$ number of distinct frequencies in $\bdfone$;
            \For{$r = 1, \cdots, R$}
                \State $\mathbb{C}^{h_r} \ni \bdztwo_r \gets \textsc{Eigenvalues}(\symbf{G}_r)$\footnotemark;
                \Comment{See \cref{eq:block-G}.}
            \EndFor
        \EndIf
        \State $
            \bdftwo,
            \bdetatwo \gets
            \left(
                \nicefrac{\fswtwo}{2 \pi}
            \right)
            \Im \left( \ln \symbf{z}^{(2)} \right) + \fofftwo,
            -\fswtwo \Re \left( \ln \symbf{z}^{(2)} \right)
        $;
        \algstore{mmempm}
    \end{algorithmic}
\end{algorithm}
\footnotetext{
    N.B. Here it is assumed that $\symbf{G}$ is block-diagonal, in accordance
    with \cref{eq:block-G}. In practice $\symbf{G}$ may not be block-diagonal,
    as whether it is or not is dependent on the ordering of the rows and
    columns in the matrix. A more explicit depiction of how $\symbf{G}_r$ can
    be extracted from $\symbf{G}$ is provided by \cref{lst:mmempm} (Lines
    \ref{ln:similarfstart} to \ref{ln:similarfend}).
}
\begin{algorithm}
    \begin{algorithmic}
        \algrestore{mmempm}
        \State $
        \symbf{Z}^{(2)}_{\text{L}} =
        \begin{bmatrix}
            \symbf{1} &
            \bdztwo &
            {\bdztwo}^2 &
            \cdots &
            {\bdztwo}^{\Ltwo-1}
        \end{bmatrix}\T
        $;
        \State $
            \symbf{Z}^{(2)}_{\text{R}} \gets
            \begin{bmatrix}
                \symbf{1} & \bdztwo & {\bdztwo}^2 & \cdots & {\bdztwo}^{\Ntwo - \Ltwo}
            \end{bmatrix}
        $;
        \State $\symbf{Z}^{(1)}_{\text{D}} \gets \diag\left(\bdzone\right)$;
       \State $
            \symbf{E}_{\text{L}} \gets
            \begin{bmatrix}
                \symbf{Z}^{(2)}_{\text{L}} \\
                \symbf{Z}^{(2)}_{\text{L}} \symbf{Z}^{(1)}_{\text{D}} \\
                \vdots\\
                \symbf{Z}^{(2)}_{\text{L}} \left[\symbf{Z}^{(1)}_{\text{D}}\right]^{\Lone - 1}
            \end{bmatrix}
        $;
        \State $
            \symbf{E}_{\text{R}} \gets
            \begin{bmatrix}
                \symbf{Z}^{(2)}_{\text{R}} &
                \symbf{Z}^{(1)}_{\text{D}} \symbf{Z}^{(2)}_{\text{R}} &
                \cdots &
                \left[\symbf{Z}^{(1)}_{\text{D}}\right]^{\None - \Lone} \symbf{Z}^{(2)}_{\text{R}}
            \end{bmatrix}
        $;
        \State $
           \bdalpha \gets \diag
           \left(
               \symbf{E}_{\text{L}}^+
               \symbf{E}_{\symbf{Y}}^{\vphantom{+}}
               \symbf{E}_{\text{R}}^+
           \right)
           $;
        \State $
            \bda, \bdphi \gets
            \left\lvert \bdalpha \right\rvert,
            \arctan \left( \frac{\Im\left(\bdalpha\right)}{\Re\left(\bdalpha\right)} \right)
        $;
        \State $\symbf{\theta}^{(0)} \gets
        \begin{bmatrix}
            \bda\T &
            \bdphi\T &
            \left[\symbf{f}^{(1)}\right]\T &
            \left[\symbf{f}^{(2)}\right]\T &
            \left[\symbf{\eta}^{(1)}\right]\T &
            \left[\symbf{\eta}^{(2)}\right]\T
        \end{bmatrix}
        ^{\mathrm{T}}$;
        \State \textbf{return} $\symbf{\theta}^{(0)}$
    \EndProcedure
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption[
        \acs{ST} method for determining an update in \acs{NLP}.
    ]{
        \ac{ST} method for determining an update in \ac{NLP}.
        This is equivalent to Algorithm 7.2 in \cite{Nocedal2006}.
    }
    \label{alg:steihaug-toint}
    \begin{algorithmic}[1]
        \Procedure{SteihaugToint}{
            $\bY \in \mathbb{C}^{\None \times \cdots \times \ND},
            \bthk \in \mathbb{R}^{2(1 + D)M},
            \trustradius{k} \in \mathbb{R}_{>0}
            $
        }
            \State $\symbf{g} \gets \nabla \FphithkY$;
            \Comment{Grad vector: \cref{eq:grad}}
            \State $\symbf{H} \gets \nabla^2 \FphithkY$;
            \Comment{Hessian matrix, either exact: \cref{eq:hess} or approximate: \cref{eq:hess-approx}}
            \State $\epsilon^{(k)} \gets \min\left(
                    \nicefrac{1}{2},
                    \sqrt{\left\lVert \symbf{g} \right\rVert}
                \right)
                \left\lVert \symbf{g} \right\rVert
                $;
            \State $\symbf{z}^{(0)} \gets \symbf{0} \in \mathbb{R}^{6M}$;
            \State $\symbf{r}^{(0)} \gets \symbf{g}$;
            \State $\symbf{d}^{(0)} \gets -\symbf{r}^{(0)}$;
            \If {$\left \lVert \symbf{r}^{(0)} \right \rVert < \epsilon^{(k)}$}
                \State \textbf{return} $\symbf{z}^{(0)}$;
            \EndIf
            \For {$j = \lbrace 0, 1, \cdots \rbrace$}
                \If {
                    ${\symbf{d}^{(j)}}^{\mathrm{T}}
                    \symbf{H}
                    \symbf{d}^{(j)}
                    \leq 0$
                }
                \State Find $\tau$ such that $\symbf{p}^{(k)} = \symbf{z}^{(j)} + \tau \symbf{d}^{(j)}$
                    minimises $\FphiQthkpk$, subject to
                    $\left \lVert \symbf{p}^{(k)} \right \rVert = \trustradius{k}$;
                    \State \textbf{return} $\symbf{p}^{(k)}$;
                \EndIf
                \State $\alpha^{(j)} \gets \dfrac
                    {{\symbf{r}^{(j)}}^{\mathrm{T}} \symbf{r}^{(j)}}
                    {
                        {\symbf{d}^{(j)}}^{\mathrm{T}}
                        \symbf{H}
                        \symbf{d}^{(j)}
                    }$;
                \State $\symbf{z}^{(j+1)} \gets \symbf{z}^{(j)} + \alpha^{(j)} \symbf{d}^{(j)}$;
                \If {$\left \lVert \symbf{z}^{(j+1)} \right \rVert < \epsilon^{(k)}$}
                    \State Find $\tau \in \mathbb{R}_{>0}$ such that
                        $\symbf{p}^{(k)} = \symbf{z}^{(j)} + \tau \symbf{d}^{(j)}$
                        satisfies
                        $\left \lVert \symbf{p}^{(k)} \right \rVert = \trustradius{k}$;
                    \State \textbf{return} $\symbf{p}^{(k)}$;
                \EndIf
                \State $\symbf{r}^{(j+1)} \gets
                    \symbf{r}^{(j)} +
                    \alpha^{(j)}
                    \symbf{H}
                    \symbf{d}^{(j)}$;
                    \If{$\left\lVert \symbf{r}^{(j+1)} \right \rVert < \epsilon^{(k)}$}
                    \State \textbf{return} $\symbf{z}^{(j+1)}$;
                \EndIf
                \State $\beta^{(j+1)} \gets
                    \dfrac{{\symbf{r}^{(j+1)}}^{\mathrm{T}} \symbf{r}^{(j+1)}}{{\symbf{r}^{(j)}}^{\mathrm{T}} \symbf{r}^{(j)}}$;
                \State $\symbf{d}^{(j+1)} \gets -\symbf{r}^{(j+1)} + \beta^{(j+1)} \symbf{d}^{(j)}$;
            \EndFor
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[h!]
    \begin{algorithmic}[1]
        \caption[
            Filtering procedure for 2D data.
        ]{
            Filtering procedure for 2D data. The arguments
            $l/r_{\text{I/N}}^{(1/2)}$ denote the left ($l$)/right ($r$) bound
            of the region of interest (I)/noise region (N) in dimension 1/2.
            These arguments should be members of the set $\lbrace 0, \cdots,
            \None - 1/\Ntwo - 1\rbrace$.
        }
        \label{alg:filter-2d}
        \Procedure{Filter$2$D}{$\bY_{\cos} \in \mathbb{C}^{\None \times \Ntwo}, \bY_{\sin} \in \mathbb{C}^{\None \times \Ntwo},
            l_{\text{I}}^{(1)}, r_{\text{I}}^{(1)}, l_{\text{I}}^{(2)}, r_{\text{I}}^{(2)},
            l_{\text{N}}^{(1)}, r_{\text{N}}^{(1)}, l_{\text{N}}^{(2)}, r_{\text{N}}^{(2)}
        $}
            \State $\bY_{\text{VE}} \gets \textsc{VirtualEcho$2$D}\left(\bY_{\cos}, \bY_{\sin}\right)$;
            \State $\symbf{S}_{\text{VE}} \gets \FT\left(\bY_{\text{VE}}\right)$;
           \For{$d = 1, 2$}
                \State $c^{(d)} \gets \nicefrac{\left(l_{\text{I}}^{(d)} + r_{\text{I}}^{(d)}\right)}{2}$;
                \State $b^{(d)} \gets r_{\text{I}}^{(d)} - l_{\text{I}}^{(d)}$;
                \State $\symbf{g}^{(d)} \gets \textsc{SuperGaussian$1$D}\left( 2\Nd_{\vphantom{\text{idx}}}, c^{(d)}, b^{(d)} \right)$;
                \Comment{See \cref{alg:filter-1d}.}
                \State $\symbf{G} \gets  \symbf{g}^{(1)} \otimes \symbf{g}^{(2)}$;
            \EndFor
            \State $\symbf{S}_{\text{N}} \gets \symbf{S}_{\text{VE}} \left[
                    l^{(1)}_{\text{N}} :
                    r^{(1)}_{\text{N}} + 1,
                    l^{(2)}_{\text{N}} :
                    r^{(2)}_{\text{N}} + 1
                \right] $
            \State $\sigma^2 \gets \Var\left(\symbf{S}_{\text{N}}\right)$;
            \State $\symbf{W}_{\sigma^2} \gets \symbf{0} \in \mathbb{R}^{2\None \times 2\Ntwo}$;
            \For {$\none = 0, \cdots, 2\None - 1$}
                \For {$\ntwo = 0, \cdots, 2\Ntwo - 1$}
                \State $w_{\sigma^2, \none, \ntwo} \gets \textsc{RandomSample}\left(\mathcal{N}\left(0, \sigma^2\right)\right)$;
                \EndFor
            \EndFor
            \State $\widetilde{\symbf{S}}_{\text{VE}} \gets \symbf{S}_{\text{VE}} \odot \symbf{G} + \symbf{W}_{\sigma^2} \odot \left(\symbf{1} - \symbf{G}\right)$;
            \State $\widetilde{\symbf{Y}}_{\text{VE}} \gets \IFT \left( \widetilde{\symbf{S}}_{\text{VE}} \right)$;
            \State $\widetilde{\symbf{Y}} \gets \widetilde{\symbf{Y}}_{\text{VE}}\left[ :\None, :\Ntwo \right]$;
            \State \textbf{return} $\widetilde{\symbf{Y}}$;
        \EndProcedure
        \Statex
        \Procedure{VirtualEcho$2$D}{$\bY_{\cos} \in \mathbb{C}^{\None \times \Ntwo}, \bY_{\sin} \in \mathbb{C}^{\None \times \Ntwo} $}
            \State $\symbf{\Psi}_{++} \gets
                \Re\left(\bY_{\cos}\right) -
                \Im\left(\bY_{\sin}\right) + \iu \left(
                \Im\left(\bY_{\cos}\right) +
                \Re\left(\bY_{\sin}\right)
            \right)
            $;
            \State $\symbf{\Psi}_{+-} \gets
                \Re\left(\bY_{\cos}\right) +
                \Im\left(\bY_{\sin}\right) + \iu \left(
                \Re\left(\bY_{\sin}\right) -
                \Im\left(\bY_{\cos}\right)
            \right)
            $;
            \State $\symbf{\Psi}_{-+} \gets
                \Re\left(\bY_{\cos}\right) +
                \Im\left(\bY_{\sin}\right) + \iu \left(
                \Im\left(\bY_{\cos}\right) -
                \Re\left(\bY_{\sin}\right)
            \right)
            $;
            \State $\symbf{\Psi}_{--} \gets
                \Re\left(\bY_{\cos}\right) -
                \Im\left(\bY_{\sin}\right) - \iu \left(
                \Im\left(\bY_{\cos}\right) +
                \Re\left(\bY_{\sin}\right)
            \right)
            $;
            \State $\symbf{Z} \gets \symbf{0} \in \mathbb{C}^{\None \times \Ntwo}$
            \State $\symbf{T}_1 \gets
                \begin{bmatrix}
                    \symbf{\Psi}_{++} & \symbf{Z} \\
                    \symbf{Z} & \symbf{Z}
                \end{bmatrix}
            $;
            \State $\symbf{T}_2 \gets
                \begin{bmatrix}
                    \symbf{Z} & \symbf{\Psi}_{+-}^{\leftrightsquigarrow (2)} \\
                    \symbf{Z} & \symbf{Z}
                \end{bmatrix}^{\circlearrowright (2)}
            $;
            \State $\symbf{T}_3 \gets
                \begin{bmatrix}
                    \symbf{Z} & \symbf{Z}\\
                    \symbf{\Psi}_{-+}^{\leftrightsquigarrow (1)} & \symbf{Z}
                \end{bmatrix}^{\circlearrowright (1)}
            $;
            \State $\symbf{T}_4 \gets
                \begin{bmatrix}
                    \symbf{Z} & \symbf{Z}\\
                    \symbf{Z} & \symbf{\Psi}_{--}^{\leftrightsquigarrow (1,2)}
                \end{bmatrix}^{\circlearrowright (1,2)}
            $;
            \State $\bY_{\text{VE}} \gets \symbf{T}_1 + \symbf{T}_2 + \symbf{T}_3 + \symbf{T}_4$;
            \For{$\none = 0, \cdots, 2 \None - 1$}
                \State $y_{\text{VE}, \none, 0} \gets
                    \nicefrac{\by_{\text{VE},\none, 0}}{2}
                $;
            \EndFor
            \For{$\ntwo = 0, \cdots, 2 \Ntwo - 1$}
                \State $y_{\text{VE}, \ntwo, 0} \gets
                    \nicefrac{\by_{\text{VE},\ntwo, 0}}{2}
                $;
            \EndFor
            \State \textbf{return} $\bY_{\text{VE}}$;
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

% \begin{algorithm}[h!]
%     \begin{algorithmic}[1]
%         \caption{Filtering procedure for 2DJ data.}
%         \label{alg:filter-2dj}
%         \Procedure{Filter$2$DJ}{
%             $\bY \in \mathbb{C}^{\None \times \Ntwo},
%             \symbf{r}_{\text{interest}} \in \mathbb{N}_0^2,
%             \symbf{r}_{\text{noise}} \in \mathbb{N}_0^2$
%         }
%             \State $\bY_{\text{VE}} \gets \symbf{0} \in \mathbb{C}^{\None \times 2 \Ntwo}$;
%             \For{$\none = 0, \cdots, \None - 1$}
%                 \State $\bY_{\text{VE}}\left[\none, :\right] \gets \textsc{VirtualEcho$1$D}\left(\bY\left[\none, :\right]\right)$;
%             \EndFor
%             \State $\symbf{S}_{\text{VE}} \gets \FT^{(2)}\left(\bY_{\text{VE}}\right)$;
%             \State $l^{(2)}_{\text{idx}}, r^{(2)}_{\text{idx}} \gets \symbf{r}_{\text{interest}}[0], \symbf{r}_{\text{interest}}[1]$;
%             \State $l^{(2)}_{\text{idx,noise}}, r^{(2)}_{\text{idx,noise}} \gets \symbf{r}_{\text{noise}}[0], \symbf{r}_{\text{noise}}[1]$;
%             \State $c_{\text{idx}}^{(2)} \gets \nicefrac{\left(l_{\text{idx}}^{(2)} + r_{\text{idx}}^{(2)}\right)}{2}$;
%             \State $b_{\text{idx}}^{(2)} \gets r_{\text{idx}}^{(2)} - l_{\text{idx}}^{(2)}$;
%             \State $\symbf{g}^{(1)} \gets \symbf{1} \in \mathbb{R}^{\None}$;
%             \State $\symbf{g}^{(2)} \gets \textsc{SuperGaussian$1$D}\left(\Ntwo_{\vphantom{\text{idx}}}, c_{\text{idx}}^{(2)}, b_{\text{idx}}^{(2)}\right)$;
%             \State $\symbf{G} \gets \symbf{g}^{(1)} \otimes \symbf{g}^{(2)}$;
%             \State $\symbf{S}_{\text{noise}} \gets \symbf{S}_{\text{VE}} \left[
%                 :, l^{(2)}_{\text{idx,noise}} : r^{(2)}_{\text{idx,noise}} + 1
%             \right]
%             $;
%             \State $\sigma^2 \gets \Var\left(\symbf{S}_{\text{noise}}\right)$;
%             \State $\symbf{W}_{\sigma^2} \gets \symbf{0} \in \mathbb{R}^{\None \times 2 \Ntwo}$;
%             \For {$\none = 0, \cdots, \None - 1$}
%                 \For {$\ntwo = 0, \cdots, 2\Ntwo - 1$}
%                     \State $\symbf{W}_{\sigma^2}\left[ \none, \ntwo \right] \gets \textsc{RandomSample}\left(\mathcal{N}\left(0, \sigma^2\right)\right)$;
%                 \EndFor
%             \EndFor
%             \State $\widetilde{\symbf{S}}_{\text{VE}} \gets \symbf{S}_{\text{VE}} \odot \symbf{G} + \symbf{W}_{\sigma^2} \odot \left(\symbf{1} - \symbf{G}\right)$;
%             \State $\widetilde{\symbf{Y}}_{\text{VE}} \gets \IFT^{(2)} \left( \widetilde{\symbf{S}}_{\text{VE}} \right)$;
%             \State $\widetilde{\symbf{Y}} \gets \widetilde{\symbf{Y}}_{\text{VE}}\left[ :, :\Ntwo \right]$;
%             \State \textbf{return} $\widetilde{\symbf{Y}}$;
%         \EndProcedure
%     \end{algorithmic}
% \end{algorithm}
