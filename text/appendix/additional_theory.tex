\chapter{Additional Theory}
\label{chap:additional-theory}

\section{Mathematical definitions}

Descriptions of linear algebra and statics concepts that are referred to in
this thesis are provided here. More detail can be found in numerous relevant
texts\cite{Strang2018,Pawitan2001}.

\subsection{Linear algebra}
\label{subsec:linear-algebra}

\note{TODO: null space, rank, range}

\subsubsection{\Acl{SVD}}
\ac{SVD} is a generalisation of eigendecomposition for a matrix of any shape.
Given a matrix $\symbf{A} \in \mathbb{C}^{m \times n}$, the \ac{SVD} is a
factorisation given by
\begin{equation}
    \symbf{A} = \symbf{U} \symbf{\Sigma} \symbf{V}^{\dagger}.
\end{equation}
The matrices that make up the decomposition are as follows
\begin{itemize}[label={}]
    \item $\symbf{\Sigma} \in \mathbb{C}^{m \times n}$ is a rectangular
        diagonal matrix with diagonal elements comprising the \emph{singular
        values} in descending order of magnitude. The singular values are the
        square roots of the non-zero eigenvalues of both
        $\symbf{A}^{\dagger}\symbf{A}$ and
        $\symbf{A}\symbf{A}^{\dagger}$.
    \item $\symbf{U} \in \mathbb{C}^{m \times m}$ is a unitary matrix whose columns
        comprise the \emph{left singular vectors}. The left singular vectors
        are the eigenvectors of the matrix $\symbf{A}\symbf{A}^{\dagger}$.
    \item $\symbf{V} \in \mathbb{C}^{n \times n}$ is a unitary matrix whose columns
        comprise the \emph{right singular vectors}. The right singular vectors
        are the eigenvectors of the matrix $\symbf{A}^{\dagger}\symbf{A}$.
\end{itemize}
One fundamental property of the \ac{SVD} is that the number of non-zero
singular values is equivalent to the rank of the matrix. As the \ac{EYM}
theorem highlights, the \ac{SVD} is valuable in constructing low-rank
approximations of matrices, with applications in various fields such as signal
processing, (see \cref{subsec:mpm}) and data compression.

\subsubsection{Special matrices}
A \emph{Hankel matrix} is a matrix in which each ascending diagonal from left to right
possesses identical elements. While Hankel matrices are often defined to be square, in
this work such a restriction is not applied. Given a matrix $\symbf{X} \in
\mathbb{F}^{M \times N}$, the matrix is Hankel if
\begin{equation}
    x_{m,n} = x_{m+1,n-1}
\end{equation}
$\forall m \in \lbrace 1, \cdots, M-1 \rbrace
\ \forall n \in \lbrace 2, \cdots, N \rbrace$.
Similarly, a \emph{Toeplitz matrix} is a matrix in which every descending
diagonal from left to right possesses identical elements, i.e.
\begin{equation}
    x_{m,n} = x_{m+1,n+1}
\end{equation}
$\forall m \in \lbrace 1, \cdots, M-1 \rbrace\ \forall n
\in \lbrace 1, \cdots N-1, \rbrace$.

\subsection{Statistics and probability}

\note{TODO: Likelihood function, MLE}

\subsubsection{\Acl{pdf}}
The \ac{pdf} $p(x) : \mathbb{R} \rightarrow \mathbb{R}$ is a function over a
continuous sample space which provides relative likelihoods between potential
values of $x$.  The probability that a random sample obeying a known
distribution lies withing the range $[x_a, x_b]$ is given by the integral
\begin{equation}
    P(x_a \leq x \leq x_b) = \int_{x_a}^{x_b} p(x) \mathrm{d}x.
\end{equation}
The integral of $p(x)$ over the entire sample space is defined to be
unity. Also, the \ac{pdf} of a specific value is always $0$, since the width of
the region of integration is $0$.

\subsubsection{Likelihood function}

\section{Estimation Errors}
\label{subsec:errors}
A measure of the degree of uncertainty in the parameter estimates can be obtained
by computing the \emph{standard errors} associated with the \ac{NLP} routine.
Standard errors are related  to the \emph{observed Fisher information matrix}
at convergence\cite[Section 2.7]{Pawitan2001}:
\begin{equation}
    \symbf{\epsilon}\left(\bthstar\right) = \sqrt{\diag\left(\symbf{I}\left( \bthstar \right)^{-1}\right)},
\end{equation}
where the observed Fisher Information matrix contains the negative partial second
derivatives of the log-likelihood with respect to $\bth$:
\begin{equation}
    \symbf{I}\left(\bth\right)_{i, j} =
        -\frac
        {\partial^2 \ell \left( \bdthY \right)}
        {\partial \theta_i \partial \theta_j}.
\end{equation}
Recalling the form of the log-likelihood given by \cref{eq:log-likeihood},
the elements of $\symbf{I}\left(\bth\right)$ are
\begin{equation}
    \symbf{I}\left(\bth\right)_{i, j} =
        -\frac{1}{\sigma^2}
        \Re
        \biggl(
            \left\langle
                \frac{\partial \bX}{\partial \theta_i},
                \frac{\partial \bX}{\partial \theta_j}
            \right\rangle
            -
            \left\langle
                \left(\bY - \bX\right),
                \frac{\partial^2 \bX}{\partial \theta_i \partial \theta_j}
            \right\rangle
        \biggl),
\end{equation}
which very closely resembles the Hessian of $\bth$:
\begin{equation}
    \symbf{I}\left(\bth\right)_{i,j} =
        \frac{1}{2 \sigma^2}
            \bdHth_{i,j}.
\end{equation}
The standard errors therefore take the form
\begin{equation}
    \symbf{\epsilon}\left(\bthstar\right) =
        \sqrt{
            2\sigma^2 \diag \left(
                \bdHthstar^{-1}
            \right)
        }.
\end{equation}
The mean and variance of the noise are $0$ and $2\sigma^2$, respectively,
leading to:
\begin{equation}
    2 \sigma^2 = \frac{1}{\mathfrak{N} - 1}
    \left\lVert \bW \right\rVert^2 =
    \frac{1}{\mathfrak{N} - 1} \left \lVert
        \bY - \bXthstar
    \right \rVert^2.
\end{equation}
Finally a useable expression for the standard errors is arrived at:
\begin{equation}
    \symbf{\epsilon}\left(\bthstar\right) =
        \sqrt{
            \frac
            {
                \Fthstar \diag \left(
                    \bdHthstar^{-1}
                \right)
            }
            {\mathfrak{N} - 1}
        }
\end{equation}



\section{Multidimensional \aclp{VE}}
\label{sec:multidim-ve}
The \ac{VE} concept (\cref{subsec:ve}) can be generalised to any number
of dimensions, assuming that a pair of amplitude-modulated signals exist for
each indirect-dimension. Thus a set of $2^{D-1}$ signals is required for a
$D$-dimensional \ac{FID}.
For the \ac{2D} case, this corresponds to the pair of signals $\lbrace
\bY^{\cos}, \bY^{\sin} \rbrace$, given by \cref{eq:general-fid} with $D=2$ and
$\zeta = \lbrace \cos(\cdot), \sin(\cdot) \rbrace$, taking the forms (with
noise neglected)
\begin{subequations}
    \begin{gather}
        y^{\cos}_{\none,\ntwo} =
            \xi^{\vphantom{(1)}}_{\none,\ntwo}
            c^{(1)}_{\none,\ntwo}
            \left(c^{(2)}_{\none,\ntwo} + \iu s^{(2)}_{\none,\ntwo}\right),\\
        y^{\sin}_{\none,\ntwo} =
            \xi^{\vphantom{(1)}}_{\none,\ntwo}
            s^{(1)}_{\none,\ntwo}
            \left(c^{(2)}_{\none,\ntwo} + \iu s^{(2)}_{\none,\ntwo}\right),\\
        \xi^{\vphantom{(1)}}_{\none,\ntwo} =
            \sum_m a_m \exp\left(-\etaonem \none \Dtone -\etatwom \ntwo \Dttwo\right),\\
        (c/s)^{(1/2)}_{\none,\ntwo} =
            \sum_m \cos / \sin \left(2 \pi f^{(1/2)} n^{(1/2)} \Updelta^{(1/2)}\right).
    \end{gather}
\end{subequations}
Four matrices $\symbf{\psi}_{\pm\pm}$ are then constructed of the form
\begin{equation}
    \begin{gathered}
        \psi_{\pm\pm, \none, \ntwo} =
            \xi_{\none,\ntwo}^{\vphantom{(1)}}
            \left(c^{(1)}_{\none,\ntwo} \pm^{(1)} \iu s^{(1)}_{\none,\ntwo}\right)
            \left(c^{(2)}_{\none,\ntwo} \pm^{(2)} \iu s^{(2)}_{\none,\ntwo}\right)\\
         \equiv
             \Re\left( y^{\cos}_{\none,\ntwo} \right)
             \pm^{(1)} \pm^{(2)} -
             \Im\left( y^{\sin}_{\none,\ntwo} \right)
             + \iu \left(
             \pm^{(1)}
             \Re\left( y^{\sin}_{\none,\ntwo} \right)
             \pm^{(2)}
             \Im\left( y^{\cos}_{\none,\ntwo} \right)
             \right),
    \end{gathered}
\end{equation}
from which the matrices $\symbf{T}_{1 \rightarrow 4} \in \mathbb{C}^{2 \None
\times 2 \Ntwo}$ are generated:
\begin{subequations}
    \begin{gather}
        \symbf{T}_1 =
        \begin{bmatrix}
            \symbf{\Psi}_{++} & \symbf{0} \\
            \symbf{0} & \symbf{0}
        \end{bmatrix}, \\
        \symbf{T}_2 =
        \begin{bmatrix}
            \symbf{0} & \symbf{0} \\
            \symbf{\Psi}_{-+}^{\leftrightsquigarrow (1)} & \symbf{0}
        \end{bmatrix}^{\circlearrowright (1)}, \\
        \symbf{T}_3 =
        \begin{bmatrix}
            \symbf{0} & \symbf{\Psi}_{+-}^{\leftrightsquigarrow (2)} \\
            \symbf{0} & \symbf{0}
        \end{bmatrix}^{\circlearrowright (2)}, \\
        \symbf{T}_4 =
        \begin{bmatrix}
            \symbf{0} & \symbf{0} \\
            \symbf{0} & \symbf{\Psi}_{--}^{\leftrightsquigarrow (1,2)}
        \end{bmatrix}^{\circlearrowright (1,2)}.
    \end{gather}
\end{subequations}
The virtual echo is then given by $\symbf{Y}_{\text{ve}} = \sum_{i=1}^4
\symbf{T}_i$, with the first row and column divided by two. For a full outline
of the 2D filtering procedure, see \cref{alg:filter-2d}.

It is possible to construct a virtual echo using an appropriate set of
phase-modulated signals too, which for the \ac{2D} case would be $\lbrace
\symbf{Y}^{\text{pos}}, \symbf{Y}^{\text{neg}}\rbrace$, given by
\cref{eq:general-fid} with $D=2$ and  $\zeta = \lbrace \exp(\iu \cdot),
\exp(-\iu\cdot)\rbrace$. These can be used to generate an amplitude modulated pair via
\begin{subequations}
    \begin{gather}
        \symbf{Y}^{\text{cos}} = \frac{\symbf{Y}^{\text{pos}} + \symbf{Y}^{\text{neg}}}{2},\\
        \symbf{Y}^{\text{sin}} = \frac{\symbf{Y}^{\text{pos}} - \symbf{Y}^{\text{neg}}}{2\iu}.
    \end{gather}
\end{subequations}

\section{Additional algorithms}
\note{Last minute edit: manually rearrange algorithms so a masses of white space aren't present}
\label{sec:algs}
\begin{algorithm}[h!]
    \caption[
        The \acs{MMEMPM}.
    ]{
        The \acs{MMEMPM}. \textsc{TruncatedSVD} is a routine which computes the
        first $M$ \ac{SVD} components of a matrix.
    }
    \label{alg:mmempm}
    \begin{algorithmic}[1]
        \Procedure {MMEMPM}{$\symbf{Y} \in \mathbb{C}^{\None \times \Ntwo}, M \in \mathbb{N}$}
        \State $\Lone, \Ltwo \gets \left\lfloor \nicefrac{\None}{2} \right\rfloor, \left\lfloor \nicefrac{\Ntwo}{2} \right\rfloor$;
        \For{$\none \gets \lbrace 0, \cdots, \None - 1 \rbrace$}
            \State  $\symbf{H}_{\by,\none} \gets
                \def\arraystretch{1.4}
            \begin{bmatrix}
                y_{\none, 0} &
                y_{\none, 1} &
                \cdots &
                y_{\none, \Ntwo-L^{(2)}}\\
                y_{\none, 1} &
                y_{\none, 2} &
                \cdots &
                y_{\none, \Ntwo-L^{(2)}+1}\\
                \vdots & \vdots & \ddots & \vdots\\
                y_{\none, L^{(2)} - 1} &
                y_{\none, L^{(2)}} &
                \cdots &
                y_{\none, \Ntwo-1}
            \end{bmatrix}
        $;
        \EndFor
        \State $\symbf{E}_{\symbf{Y}} \gets
        \begin{bmatrix}
            \symbf{H}_{\by,0} & \symbf{H}_{\by,1} & \cdots & \symbf{H}_{\by,\None - L^{(1)}}\\
            \symbf{H}_{\by,1} & \symbf{H}_{\by,2} & \cdots & \symbf{H}_{\by,\None - L^{(1)} + 1}\\
            \vdots & \vdots & \ddots & \vdots\\
            \symbf{H}_{\by,L^{(1)} - 1} & \symbf{H}_{\by,L^{(1)}} & \cdots & \symbf{H}_{\by,\None - 1}
        \end{bmatrix}
        $;
        \State $\symbf{U}_M^{\vphantom{\dagger}},
            \symbf{\Sigma}_M^{\vphantom{\dagger}},
            \symbf{V}_M^{\dagger} \gets
            \textsc{TruncatedSVD}\left(\EY, M\right)$;
        \State $\symbf{P} \gets \symbf{0} \in \mathbb{C}^{\Lone \Ltwo \times \Lone \Ltwo}$;
        \State $r \gets 0$
        \For{$i = 0, \cdots, \Ltwo - 1$}
            \For{$j = 0, \cdots, \Lone - 1$}
                \State $c \gets i + j \Ltwo$;
                \State $p_{r, c} \gets 1$;
                \State $r \gets r + 1$;
            \EndFor
        \EndFor
        \State $\symbf{U}_{M1}, \symbf{U}_{M2} \gets \symbf{U}_M\left[ : L^{(1)}(L^{(2)}-1)\right], \symbf{U}_M\left[L^{(2)}:\right]$;
        \Comment{Last/First $L^{(2)}$ rows deleted}
        \State $\symbf{z}^{(1)}, \symbf{W}^{(1)} \gets \textsc{Eigendecomposition}\left( \symbf{U}_{M1}^+ \symbf{U}_{M2}^{\vphantom{+}} \right)$;
        \State $
            \symbf{f}^{(1)},
            \symbf{\eta}^{(1)} \gets
            \left(
                \nicefrac{f_{\text{sw}}^{(1)}}{2 \pi}
            \right)
            \Im \left( \ln \symbf{z}^{(1)} \right) + \foffone,
            -\fswone \Re \left( \ln \symbf{z}^{(1)} \right)
        $;
        \State $\symbf{U}_{M\text{P}} \gets \symbf{P} \symbf{U}_M$;
        \State $
            \symbf{U}_{M\text{P}1},
            \symbf{U}_{M \text{P} 2} \gets
            \symbf{U}_{M \text{P}}\left[ : (L^{(1)}-1)L^{(2)}\right],
            \symbf{U}_{M \text{P}}\left[L^{(1)}:\right]
        $;
        \Comment{Last/First $L^{(1)}$ rows deleted}
        \State $
            \symbf{G} \gets
                \left[\symbf{W}^{(1)}\right]^{-1}
                \symbf{U}_{M\text{P}1}^{+}
                \symbf{U}_{M\text{P}2}^{\vphantom{+}}
                \symbf{W}^{(1)}
            $;
        \If{all values in $\bdfone$ are distinct}
        \Comment{See \cref{fn:similar-freqs}, \cpageref{fn:similar-freqs}}
            \State $\bdztwo \gets \diag(\symbf{G})$;
        \Else
            \State $R \gets$ number of distinct frequencies in $\bdfone$;
            \For{$r = 1, \cdots, R$}
                \State $\mathbb{C}^{h_r} \ni \bdztwo_r \gets \textsc{Eigenvalues}(\symbf{G}_r)$\footnotemark;
                \Comment{See \cref{eq:block-G}.}
            \EndFor
        \EndIf
        \State $
            \bdftwo,
            \bdetatwo \gets
            \left(
                \nicefrac{\fswtwo}{2 \pi}
            \right)
            \Im \left( \ln \symbf{z}^{(2)} \right) + \fofftwo,
            -\fswtwo \Re \left( \ln \symbf{z}^{(2)} \right)
        $;
        \algstore{mmempm}
    \end{algorithmic}
\end{algorithm}
\footnotetext{
    N.B. Here it is assumed that $\symbf{G}$ is block-diagonal, in accordance
    with \cref{eq:block-G}. In practice $\symbf{G}$ may not be block-diagonal,
    as whether it is or not is dependent on the ordering of the rows and
    columns in the matrix. A more explicit depiction of how $\symbf{G}_r$ can
    be extracted from $\symbf{G}$ is provided by \cref{lst:mmempm} (Lines
    \ref{ln:similar-f-start} to \ref{ln:similar-f-end}).
}
\begin{algorithm}
    \begin{algorithmic}
        \algrestore{mmempm}
        \State $
        \symbf{Z}^{(2)}_{\text{L}} =
        \begin{bmatrix}
            \symbf{1} &
            \bdztwo &
            {\bdztwo}^2 &
            \cdots &
            {\bdztwo}^{\Ltwo-1}
        \end{bmatrix}\T
        $;
        \State $
            \symbf{Z}^{(2)}_{\text{R}} \gets
            \begin{bmatrix}
                \symbf{1} & \bdztwo & {\bdztwo}^2 & \cdots & {\bdztwo}^{\Ntwo - \Ltwo}
            \end{bmatrix}
        $;
        \State $\symbf{Z}^{(1)}_{\text{D}} \gets \diag\left(\bdzone\right)$;
       \State $
            \symbf{E}_{\text{L}} \gets
            \begin{bmatrix}
                \symbf{Z}^{(2)}_{\text{L}} \\
                \symbf{Z}^{(2)}_{\text{L}} \symbf{Z}^{(1)}_{\text{D}} \\
                \vdots\\
                \symbf{Z}^{(2)}_{\text{L}} \left[\symbf{Z}^{(1)}_{\text{D}}\right]^{\Lone - 1}
            \end{bmatrix}
        $;
        \State $
            \symbf{E}_{\text{R}} \gets
            \begin{bmatrix}
                \symbf{Z}^{(2)}_{\text{R}} &
                \symbf{Z}^{(1)}_{\text{D}} \symbf{Z}^{(2)}_{\text{R}} &
                \cdots &
                \left[\symbf{Z}^{(1)}_{\text{D}}\right]^{\None - \Lone} \symbf{Z}^{(2)}_{\text{R}}
            \end{bmatrix}
        $;
        \State $
           \bdalpha \gets \diag
           \left(
               \symbf{E}_{\text{L}}^+
               \symbf{E}_{\symbf{Y}}^{\vphantom{+}}
               \symbf{E}_{\text{R}}^+
           \right)
           $;
        \State $
            \bda, \bdphi \gets
            \left\lvert \bdalpha \right\rvert,
            \arctan \left( \frac{\Im\left(\bdalpha\right)}{\Re\left(\bdalpha\right)} \right)
        $;
        \State $\symbf{\theta}^{(0)} \gets
        \begin{bmatrix}
            \bda\T &
            \bdphi\T &
            \left[\symbf{f}^{(1)}\right]\T &
            \left[\symbf{f}^{(2)}\right]\T &
            \left[\symbf{\eta}^{(1)}\right]\T &
            \left[\symbf{\eta}^{(2)}\right]\T
        \end{bmatrix}
        ^{\mathrm{T}}$;
        \State \textbf{return} $\symbf{\theta}^{(0)}$
    \EndProcedure
    \end{algorithmic}
\end{algorithm}
\vfill\null

\begin{algorithm}
    \caption[
        \acs{ST} method for determining an update in \acs{NLP}.
    ]{
        \ac{ST} method for determining an update in \ac{NLP}.
        This is equivalent to Algorithm 7.2 in \cite{Nocedal2006}.
    }
    \label{alg:steihaug-toint}
    \begin{algorithmic}[1]
        \Procedure{SteihaugToint}{
            $\bY \in \mathbb{C}^{\None \times \cdots \times \ND},
            \bthk \in \mathbb{R}^{2(1 + D)M},
            \trustradius{k} \in \mathbb{R}_{>0}
            $
        }
            \State $\symbf{g} \gets \nabla \FphithkY$;
            \Comment{Grad vector: \cref{eq:grad}}
            \State $\symbf{H} \gets \nabla^2 \FphithkY$;
            \Comment{Hessian matrix, either exact: \cref{eq:hess} or approximate: \cref{eq:hess-approx}}
            \State $\epsilon^{(k)} \gets \min\left(
                    \nicefrac{1}{2},
                    \sqrt{\left\lVert \symbf{g} \right\rVert}
                \right)
                \left\lVert \symbf{g} \right\rVert
                $;
            \State $\symbf{z}^{(0)} \gets \symbf{0} \in \mathbb{R}^{6M}$;
            \State $\symbf{r}^{(0)} \gets \symbf{g}$;
            \State $\symbf{d}^{(0)} \gets -\symbf{r}^{(0)}$;
            \If {$\left \lVert \symbf{r}^{(0)} \right \rVert < \epsilon^{(k)}$}
                \State \textbf{return} $\symbf{z}^{(0)}$;
            \EndIf
            \For {$j = \lbrace 0, 1, \cdots \rbrace$}
                \If {
                    ${\symbf{d}^{(j)}}^{\mathrm{T}}
                    \symbf{H}
                    \symbf{d}^{(j)}
                    \leq 0$
                }
                \State Find $\tau$ such that $\symbf{p}^{(k)} = \symbf{z}^{(j)} + \tau \symbf{d}^{(j)}$
                    minimises $\FphiQthkpk$, subject to
                    $\left \lVert \symbf{p}^{(k)} \right \rVert = \trustradius{k}$;
                    \State \textbf{return} $\symbf{p}^{(k)}$;
                \EndIf
                \State $\alpha^{(j)} \gets \dfrac
                    {{\symbf{r}^{(j)}}^{\mathrm{T}} \symbf{r}^{(j)}}
                    {
                        {\symbf{d}^{(j)}}^{\mathrm{T}}
                        \symbf{H}
                        \symbf{d}^{(j)}
                    }$;
                \State $\symbf{z}^{(j+1)} \gets \symbf{z}^{(j)} + \alpha^{(j)} \symbf{d}^{(j)}$;
                \If {$\left \lVert \symbf{z}^{(j+1)} \right \rVert < \epsilon^{(k)}$}
                    \State Find $\tau \in \mathbb{R}_{>0}$ such that
                        $\symbf{p}^{(k)} = \symbf{z}^{(j)} + \tau \symbf{d}^{(j)}$
                        satisfies
                        $\left \lVert \symbf{p}^{(k)} \right \rVert = \trustradius{k}$;
                    \State \textbf{return} $\symbf{p}^{(k)}$;
                \EndIf
                \State $\symbf{r}^{(j+1)} \gets
                    \symbf{r}^{(j)} +
                    \alpha^{(j)}
                    \symbf{H}
                    \symbf{d}^{(j)}$;
                    \If{$\left\lVert \symbf{r}^{(j+1)} \right \rVert < \epsilon^{(k)}$}
                    \State \textbf{return} $\symbf{z}^{(j+1)}$;
                \EndIf
                \State $\beta^{(j+1)} \gets
                    \dfrac{{\symbf{r}^{(j+1)}}^{\mathrm{T}} \symbf{r}^{(j+1)}}{{\symbf{r}^{(j)}}^{\mathrm{T}} \symbf{r}^{(j)}}$;
                \State $\symbf{d}^{(j+1)} \gets -\symbf{r}^{(j+1)} + \beta^{(j+1)} \symbf{d}^{(j)}$;
            \EndFor
        \EndProcedure
    \end{algorithmic}
\end{algorithm}



\begin{algorithm}[h!]
    \begin{algorithmic}[1]
        \caption{Filtering procedure for 2D data.}
        \label{alg:filter-2d}
        \Procedure{Filter$2$D}{$\bY_{\cos} \in \mathbb{C}^{\None \times \Ntwo}, \bY_{\sin} \in \mathbb{C}^{\None \times \Ntwo}, \symbf{R}_{\text{interest}} \in \mathbb{N}_0^{2 \times 2}, \symbf{R}_{\text{noise}} \in \mathbb{N}_0^{2 \times 2}$}
            \State $\bY_{\text{ve}} \gets \textsc{VirtualEcho$2$D}\left(\bY_{\cos}, \bY_{\sin}\right)$;
            \State $\symbf{S}_{\text{ve}} \gets \FT\left(\bY_{\text{ve}}\right)$;
            \State $
                l^{(1)}_{\text{idx}},
                r^{(1)}_{\text{idx}},
                l^{(2)}_{\text{idx}},
                r^{(2)}_{\text{idx}}
                \gets
                \symbf{R}_{\text{interest}}[0,0],
                \symbf{R}_{\text{interest}}[0,1],
                \symbf{R}_{\text{interest}}[1,0],
                \symbf{R}_{\text{interest}}[1,1]
                $;
            \For{$d = 1, 2$}
                \State $c_{\text{idx}}^{(d)} \gets \nicefrac{\left(l_{\text{idx}}^{(d)} + r_{\text{idx}}^{(d)}\right)}{2}$;
                \State $b_{\text{idx}}^{(d)} \gets r_{\text{idx}}^{(d)} - l_{\text{idx}}^{(d)}$;
                \State $\symbf{g}^{(d)} \gets \textsc{SuperGaussian$1$D}\left( 2\Nd_{\vphantom{\text{idx}}}, c_{\text{idx}}^{(d)}, b_{\text{idx}}^{(d)} \right)$;
                \State $\symbf{G} \gets  \symbf{g}^{(1)} \otimes \symbf{g}^{(2)}$;
            \EndFor
            \State $
                l^{(1)}_{\text{idx,noise}},
                r^{(1)}_{\text{idx,noise}},
                l^{(2)}_{\text{idx,noise}},
                r^{(2)}_{\text{idx,noise}}
                \gets
                \symbf{R}_{\text{noise}}[0,0],
                \symbf{R}_{\text{noise}}[0,1],
                \symbf{R}_{\text{noise}}[1,0],
                \symbf{R}_{\text{noise}}[1,1]
                $;
            \State $\symbf{S}_{\text{noise}} \gets \symbf{S}_{\text{ve}} \left[
                    l^{(1)}_{\text{idx,noise}} :
                    r^{(1)}_{\text{idx,noise}} + 1,
                    l^{(2)}_{\text{idx,noise}} :
                    r^{(2)}_{\text{idx,noise}} + 1
                \right] $
            \State $\sigma^2 \gets \Var\left(\symbf{S}_{\text{noise}}\right)$;
            \State $\symbf{W}_{\sigma^2} \gets \symbf{0} \in \mathbb{R}^{2\None \times 2\Ntwo}$;
            \For {$\none = 0, \cdots, 2\None - 1$}
                \For {$\ntwo = 0, \cdots, 2\Ntwo - 1$}
                    \State $\symbf{W}_{\sigma^2}\left[ \none, \ntwo \right] \gets \textsc{RandomSample}\left(\mathcal{N}\left(0, \sigma^2\right)\right)$;
                \EndFor
            \EndFor
            \State $\widetilde{\symbf{S}}_{\text{ve}} \gets \symbf{S}_{\text{ve}} \odot \symbf{G} + \symbf{W}_{\sigma^2} \odot \left(\symbf{1} - \symbf{G}\right)$;
            \State $\widetilde{\symbf{Y}}_{\text{ve}} \gets \IFT \left( \widetilde{\symbf{S}}_{\text{ve}} \right)$;
            \State $\widetilde{\symbf{Y}} \gets \widetilde{\symbf{Y}}_{\text{ve}}\left[ :\None, :\Ntwo \right]$;
            \State \textbf{return} $\widetilde{\symbf{Y}}$;
        \EndProcedure
        \Statex
        \Procedure{VirtualEcho$2$D}{$\bY_{\cos} \in \mathbb{C}^{\None \times \Ntwo}, \bY_{\sin} \in \mathbb{C}^{\None \times \Ntwo} $}
            \State $\symbf{\Psi}_{++} \gets
                \Re\left(\bY_{\cos}\right) -
                \Im\left(\bY_{\sin}\right) + \iu \left(
                \Im\left(\bY_{\cos}\right) +
                \Re\left(\bY_{\sin}\right)
            \right)
            $;
            \State $\symbf{\Psi}_{+-} \gets
                \Re\left(\bY_{\cos}\right) +
                \Im\left(\bY_{\sin}\right) + \iu \left(
                \Re\left(\bY_{\sin}\right) -
                \Im\left(\bY_{\cos}\right)
            \right)
            $;
            \State $\symbf{\Psi}_{-+} \gets
                \Re\left(\bY_{\cos}\right) +
                \Im\left(\bY_{\sin}\right) + \iu \left(
                \Im\left(\bY_{\cos}\right) -
                \Re\left(\bY_{\sin}\right)
            \right)
            $;
            \State $\symbf{\Psi}_{--} \gets
                \Re\left(\bY_{\cos}\right) -
                \Im\left(\bY_{\sin}\right) - \iu \left(
                \Im\left(\bY_{\cos}\right) +
                \Re\left(\bY_{\sin}\right)
            \right)
            $;
            \State $\symbf{Z} \gets \symbf{0} \in \mathbb{C}^{\None \times \Ntwo}$
            \State $\symbf{T}_1 \gets
                \begin{bmatrix}
                    \symbf{\Psi}_{++} & \symbf{Z} \\
                    \symbf{Z} & \symbf{Z}
                \end{bmatrix}
            $;
            \State $\symbf{T}_2 \gets
                \begin{bmatrix}
                    \symbf{Z} & \symbf{\Psi}_{+-}^{\leftrightsquigarrow (2)} \\
                    \symbf{Z} & \symbf{Z}
                \end{bmatrix}^{\circlearrowright (2)}
            $;
            \State $\symbf{T}_3 \gets
                \begin{bmatrix}
                    \symbf{Z} & \symbf{Z}\\
                    \symbf{\Psi}_{-+}^{\leftrightsquigarrow (1)} & \symbf{Z}
                \end{bmatrix}^{\circlearrowright (1)}
            $;
            \State $\symbf{T}_4 \gets
                \begin{bmatrix}
                    \symbf{Z} & \symbf{Z}\\
                    \symbf{Z} & \symbf{\Psi}_{--}^{\leftrightsquigarrow (1,2)}
                \end{bmatrix}^{\circlearrowright (1,2)}
            $;
            \State $\bY_{\text{ve}} \gets \symbf{T}_1 + \symbf{T}_2 + \symbf{T}_3 + \symbf{T}_4$;
            \For{$\none = 0, \cdots, 2 \None - 1$}
                \State $\bY_{\text{ve}}\left[\none, 0\right] \gets
                    \nicefrac{\bY_{\text{ve}}\left[\none, 0\right]}{2}
                $;
            \EndFor
            \For{$\ntwo = 0, \cdots, 2 \Ntwo - 1$}
                \State $\bY_{\text{ve}}\left[0, \ntwo \right] \gets
                    \nicefrac{\bY_{\text{ve}}\left[0,\hspace*{1pt} \ntwo \right]}{2}
                $;
            \EndFor
            \State \textbf{return} $\bY_{\text{ve}}$;
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[h!]
    \begin{algorithmic}[1]
        \caption{Filtering procedure for 2DJ data.}
        \label{alg:filter-2dj}
        \Procedure{Filter$2$DJ}{
            $\bY \in \mathbb{C}^{\None \times \Ntwo},
            \symbf{r}_{\text{interest}} \in \mathbb{N}_0^2,
            \symbf{r}_{\text{noise}} \in \mathbb{N}_0^2$
        }
            \State $\bY_{\text{ve}} \gets \symbf{0} \in \mathbb{C}^{\None \times 2 \Ntwo}$;
            \For{$\none = 0, \cdots, \None - 1$}
                \State $\bY_{\text{ve}}\left[\none, :\right] \gets \textsc{VirtualEcho$1$D}\left(\bY\left[\none, :\right]\right)$;
            \EndFor
            \State $\symbf{S}_{\text{ve}} \gets \FT^{(2)}\left(\bY_{\text{ve}}\right)$;
            \State $l^{(2)}_{\text{idx}}, r^{(2)}_{\text{idx}} \gets \symbf{r}_{\text{interest}}[0], \symbf{r}_{\text{interest}}[1]$;
            \State $l^{(2)}_{\text{idx,noise}}, r^{(2)}_{\text{idx,noise}} \gets \symbf{r}_{\text{noise}}[0], \symbf{r}_{\text{noise}}[1]$;
            \State $c_{\text{idx}}^{(2)} \gets \nicefrac{\left(l_{\text{idx}}^{(2)} + r_{\text{idx}}^{(2)}\right)}{2}$;
            \State $b_{\text{idx}}^{(2)} \gets r_{\text{idx}}^{(2)} - l_{\text{idx}}^{(2)}$;
            \State $\symbf{g}^{(1)} \gets \symbf{1} \in \mathbb{R}^{\None}$;
            \State $\symbf{g}^{(2)} \gets \textsc{SuperGaussian$1$D}\left(\Ntwo_{\vphantom{\text{idx}}}, c_{\text{idx}}^{(2)}, b_{\text{idx}}^{(2)}\right)$;
            \State $\symbf{G} \gets \symbf{g}^{(1)} \otimes \symbf{g}^{(2)}$;
            \State $\symbf{S}_{\text{noise}} \gets \symbf{S}_{\text{ve}} \left[
                :, l^{(2)}_{\text{idx,noise}} : r^{(2)}_{\text{idx,noise}} + 1
            \right]
            $;
            \State $\sigma^2 \gets \Var\left(\symbf{S}_{\text{noise}}\right)$;
            \State $\symbf{W}_{\sigma^2} \gets \symbf{0} \in \mathbb{R}^{\None \times 2 \Ntwo}$;
            \For {$\none = 0, \cdots, \None - 1$}
                \For {$\ntwo = 0, \cdots, 2\Ntwo - 1$}
                    \State $\symbf{W}_{\sigma^2}\left[ \none, \ntwo \right] \gets \textsc{RandomSample}\left(\mathcal{N}\left(0, \sigma^2\right)\right)$;
                \EndFor
            \EndFor
            \State $\widetilde{\symbf{S}}_{\text{ve}} \gets \symbf{S}_{\text{ve}} \odot \symbf{G} + \symbf{W}_{\sigma^2} \odot \left(\symbf{1} - \symbf{G}\right)$;
            \State $\widetilde{\symbf{Y}}_{\text{ve}} \gets \IFT^{(2)} \left( \widetilde{\symbf{S}}_{\text{ve}} \right)$;
            \State $\widetilde{\symbf{Y}} \gets \widetilde{\symbf{Y}}_{\text{ve}}\left[ :, :\Ntwo \right]$;
            \State \textbf{return} $\widetilde{\symbf{Y}}$;
        \EndProcedure
    \end{algorithmic}
\end{algorithm}
